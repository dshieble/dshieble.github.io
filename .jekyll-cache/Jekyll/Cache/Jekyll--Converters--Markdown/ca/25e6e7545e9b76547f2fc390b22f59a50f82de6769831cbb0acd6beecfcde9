I"\”<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-82391879-1', 'auto');
  ga('send', 'pageview');

</script>

<p>Generative models are awesome. TensorFlow is awesome. Music is awesome. In this post, weâ€™re going to use TensorFlow to build a generative model that can create snippets of music.</p>

<p>Iâ€™m going to assume that you have a pretty good understanding of neural networks and backpropagation and are at least a little bit familiar with TensorFlow. If you havenâ€™t used TensorFlow before, the <a href="https://www.tensorflow.org/versions/r0.10/tutorials/index.html">tutorials</a> are a good place to start.</p>

<h2 id="concepts">Concepts</h2>

<h3 id="generative-models">Generative Models</h3>

<p>Generative Models specify a probability distribution over a dataset of input vectors. For an unsupervised task, we form a model for \(P(x)\), where \(x\) is an input vector. For a supervised task, we form a model for \(P(x \vert y)\), where \(y\) is the label for \(x\). Like discriminative models, most generative models can be used for classification tasks. To perform classification with a generative model, we leverage the fact that if we know \(P(X \vert Y)\) and \(P(Y)\), we can use bayes rule to estimate \(P(Y \vert X)\).</p>

<p>Some good resources for learning about generative models include:</p>

<ul>
  <li>
    <p>A nice and <a href="http://www.cedar.buffalo.edu/~srihari/CSE574/Discriminative-Generative.pdf">simple visual description</a> of machine learning and the difference between generative and discriminative models.</p>
  </li>
  <li>
    <p>An <a href="http://cs229.stanford.edu/notes/cs229-notes2.pdf">excellent description</a> of some popular generative algorithms. My personal favorite resource. Honestly all of the notes for Andrew Ngâ€™s class are solid gold.</p>
  </li>
  <li>
    <p>A <a href="http://papers.nips.cc/paper/2020-on-discriminative-vs-generative-classifiers-a-comparison-of-logistic-regression-and-naive-bayes.pdf">heavily referenced paper</a> that compares linear discriminative and generative models. The moral of the story is that generative models tend to be better at classification when data is sparse, and discriminative classifiers are better when data is plentiful. Also written by Andrew Ng.</p>
  </li>
</ul>

<p>Unlike discriminative models, we can also use generative models to create synthetic data by directly sampling from the modelled probability distributions. I think that this is pretty sweet, and Iâ€™m going to prove it to you. But first, letâ€™s talk about RBMs.</p>

<h3 id="the-restricted-boltzman-machine">The Restricted Boltzman Machine</h3>
<p>If you have never encountered RBMs before, they can be a little complex. Iâ€™m going to quickly review the details below, but if you want understand them more thoroughly, the following resources may be helpful:</p>

<ul>
  <li>
    <p>This article contains a good, very <a href="http://deeplearning4j.org/restrictedboltzmannmachine.html">visual description of RBMs</a>. Also includes a DL4J implementation.</p>
  </li>
  <li>
    <p>This post is a relatively <a href="http://deeplearning.net/tutorial/rbm.html">in-depth description of RBMs</a>, starting from a description of Energy-Based models. Also includes an implementation in Theano.</p>
  </li>
  <li>
    <p>Here is a pretty <a href="http://blog.echen.me/2011/07/18/introduction-to-restricted-boltzmann-machines/">intuitive description</a> of RBMs as binary factor analysis. This article also includes a description of the classic movie rating RBM example.</p>
  </li>
</ul>

<h4 id="rbm-architecture">RBM Architecture</h4>
<p>The RBM is a neural network with 2 layers, the visible layer and the hidden layer. Each visible node is connected to each hidden node (and vice versa), but there are no visible-visible or hidden-hidden connections (the RBM is a complete bipartite graph). Since there are only 2 layers, we can fully describe a trained RBM with 3 parameters:</p>

<ul>
  <li>The weight matrix \(W\):
    <ul>
      <li>\(W\) has size \(n_{visible}\ x\ n_{hidden}\). \(W_{ij}\) is the weight of the connection between visible node \(i\) and hidden node \(j\).</li>
    </ul>
  </li>
  <li>The bias vector \(bv\):
    <ul>
      <li>\(bv\) is a vector with \(n_{visible}\) elements. Element \(i\) is the bias for the \(i^{th}\) visible node.</li>
    </ul>
  </li>
  <li>The bias vector bh:
    <ul>
      <li>\(bh\) is a vector with n_hidden elements. Element \(j\) is the bias for the \(j^{th}\) hidden node.</li>
    </ul>
  </li>
</ul>

<p>\(n_{visible}\) is the number of features in the input vectors. \(n_{hidden}\) is the size of the hidden layer.</p>

<p><img src="/img/RBM.png" alt="In reality RBM is fully connected" /></p>

<p>In this image we remove most of the edges for clarity.</p>

<h4 id="rbm-sampling">RBM Sampling</h4>
<p>Unlike most of the neural networks that youâ€™ve probably seen before, RBMs are generative models that directly model the probability distribution of data and can be used for data augmentation and reconstruction. To sample from an RBM, we perform an algorithm known as Gibbs sampling. Essentially, this algorithm works like this:</p>

<ul>
  <li>
    <p>Initialize the visible nodes. You can initialize them randomly, or you can set them equal to an input example.</p>
  </li>
  <li>
    <p>Repeat the following process for \(k\) steps, or until convergence:</p>
    <ol>
      <li>Propagate the values of the visible nodes forward, and then sample the new values of the hidden nodes.
        <ul>
          <li>That is, randomly set the values of each \(h_i\) to be \(1\) with probability \(\sigma (W^{T}v + bh)_i\).</li>
        </ul>
      </li>
      <li>Propagate the values of the hidden nodes back to the visible nodes, and sample the new values of the visible nodes.
        <ul>
          <li>That is, randomly set the values of each \(v_i\) to be 1 with probability \(\sigma (Wh + bv)_i\).</li>
        </ul>
      </li>
    </ol>
  </li>
</ul>

<p>At the end of the algorithm, the visible nodes will store the value of the sample.</p>

<h4 id="rbm-training">RBM Training</h4>
<p>Remember that an RBM describes a probability distribution. When we train an RBM, our goal is to find the values for its parameters that maximize the likelihood of our data being drawn from that distribution.</p>

<p>To do that, we use a very simple strategy:</p>

<ul>
  <li>Initialize the visible nodes with some vector \(x\) from our dataset</li>
  <li>Sample \(\tilde{x}\) from the probability distribution by using Gibbs sampling</li>
  <li>Look at the difference between the \(\tilde{x}\) and \(x\)</li>
  <li>Move the weight matrix and bias vectors in a direction that minimizes this difference</li>
</ul>

<p>The equations are straightforward:</p>

\[W  = W + lr(x^{T}h(x) - \tilde{x}^{T}h(\tilde{x}))\\
bh = bh + lr(h(x) - h(\tilde{x}))\\
bv = bv + lr(x - \tilde{x})\]

<ul>
  <li>\(x_t\) is the inital vector</li>
  <li>\(\tilde{x}\) is the sample of x drawn from the probability distribution</li>
  <li>\(lr\) is the learning rate</li>
  <li>\(h\) is the function that takes in the values of the visible nodes and returns a sample of the hidden nodes (see step 1 of the Gibbs sampling algorithm above)</li>
</ul>

<p>This technique is known as Contrastive Divergence. If you want to learn more about CD, you can check out <a href="http://www.robots.ox.ac.uk/~ojw/files/NotesOnCD.pdf">this excellent derivation</a>.</p>

<h2 id="implementation">Implementation</h2>

<h3 id="data">Data</h3>
<p>In this tutorial we are going to use our RBM to generate short sequences of music. Our training data will be around a hundred midi files of popular songs. Midi is a format that directly encodes musical notes - you can think of midi files as sheet music for computers. You can play midi files or convert them to mp3 by using a tool such as GarageBand. Midi files encode events that a synthesizer would need to know about, such as Note-on, Note-off, Tempo changes, etc. For our purposes, we are interested in getting the following information from our midi files:</p>

<ul>
  <li>Which note is played</li>
  <li>When the note is pressed</li>
  <li>When the note is released</li>
</ul>

<p>We can encode each song with a binary matrix with the following structure:</p>

<p><img src="/img/Matrix.png" alt="The left half encodes note on events, and the right half encodes note off events" /></p>

<p>The first n columns (where n is the number of notes that we want to represent) encode note-on events. The next n columns represent note-off events. So if element \(M_{ij}\) is 1 (where \(j&lt; n\)), then at timestep \(i\), note \(j\) is played. If element \(M_{i (n+j)}\) is 1, then at timestep \(i\), note \(j\) is released.</p>

<p>In the above encoding, each timestep is a single training example. If we reshape the matrix to concatenate multiple rows together, then we can represent multiple timesteps in a single data vector.</p>

<p>Converting midi files to and from these binary matrices is relatively simple, but there are a number of annoying edge cases. All of the code for reading and writing midi files is in the <a href="https://github.com/dshieble/Musical_Matrices/blob/master/midi_manipulation.py">midi_manipulation.py</a>  file, which is heavily based on <a href="https://github.com/hexahedria/biaxial-rnn-music-composition">Daniel Johnsonâ€™s midi manipulation code</a>. Midi files arenâ€™t really the point of this post, but if you want to know more, the following libraries are pretty useful:</p>

<ul>
  <li><a href="https://github.com/vishnubob/python-midi">python-midi</a> contains fundamental tools for reading and writing midi files in python</li>
  <li><a href="http://web.mit.edu/music21/">music21</a> is a more advanced (and complicated) midi library</li>
</ul>

<h3 id="code">Code</h3>

<p>Now letâ€™s get coding! In order for the code below to work, you need to put it in a directory with the following files:</p>

<ul>
  <li><a href="https://github.com/dshieble/Musical_Matrices/blob/master/midi_manipulation.py">midi_manipulation.py</a></li>
  <li><a href="https://github.com/dshieble/Musical_Matrices/tree/master/Pop_Music_Midi">Pop_Music_Midi</a></li>
</ul>

<p>For your convenience, all of the code is in <a href="https://github.com/dshieble/Music_RBM">this repository</a>. Just follow the instructions in the README.md.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="nn">msgpack</span>
<span class="kn">import</span> <span class="nn">glob</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="n">tf</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.ops</span> <span class="kn">import</span> <span class="n">control_flow_ops</span>
<span class="kn">from</span> <span class="nn">tqdm</span> <span class="kn">import</span> <span class="n">tqdm</span>

<span class="c1">###################################################
# In order for this code to work, you need to place this file in the same 
# directory as the midi_manipulation.py file and the Pop_Music_Midi directory
</span>
<span class="kn">import</span> <span class="nn">midi_manipulation</span>

<span class="k">def</span> <span class="nf">get_songs</span><span class="p">(</span><span class="n">path</span><span class="p">):</span>
    <span class="n">files</span> <span class="o">=</span> <span class="n">glob</span><span class="p">.</span><span class="n">glob</span><span class="p">(</span><span class="s">'{}/*.mid*'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">path</span><span class="p">))</span>
    <span class="n">songs</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="n">files</span><span class="p">):</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">song</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="n">midi_manipulation</span><span class="p">.</span><span class="n">midiToNoteStateMatrix</span><span class="p">(</span><span class="n">f</span><span class="p">))</span>
            <span class="k">if</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="n">song</span><span class="p">).</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">50</span><span class="p">:</span>
                <span class="n">songs</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">song</span><span class="p">)</span>
        <span class="k">except</span> <span class="nb">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="k">raise</span> <span class="n">e</span>           
    <span class="k">return</span> <span class="n">songs</span>

<span class="n">songs</span> <span class="o">=</span> <span class="n">get_songs</span><span class="p">(</span><span class="s">'Pop_Music_Midi'</span><span class="p">)</span> <span class="c1">#These songs have already been converted from midi to msgpack
</span><span class="k">print</span> <span class="s">"{} songs processed"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">songs</span><span class="p">))</span>
<span class="c1">###################################################
</span>
<span class="c1">### HyperParameters
# First, let's take a look at the hyperparameters of our model:
</span>
<span class="n">lowest_note</span> <span class="o">=</span> <span class="mi">24</span> <span class="c1">#the index of the lowest note on the piano roll
</span><span class="n">highest_note</span> <span class="o">=</span> <span class="mi">102</span> <span class="c1">#the index of the highest note on the piano roll
</span><span class="n">note_range</span> <span class="o">=</span> <span class="n">highest_note</span><span class="o">-</span><span class="n">lowest_note</span> <span class="c1">#the note range
</span>
<span class="n">num_timesteps</span>  <span class="o">=</span> <span class="mi">15</span> <span class="c1">#This is the number of timesteps that we will create at a time
</span><span class="n">n_visible</span>      <span class="o">=</span> <span class="mi">2</span><span class="o">*</span><span class="n">note_range</span><span class="o">*</span><span class="n">num_timesteps</span> <span class="c1">#This is the size of the visible layer. 
</span><span class="n">n_hidden</span>       <span class="o">=</span> <span class="mi">50</span> <span class="c1">#This is the size of the hidden layer
</span>
<span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">200</span> <span class="c1">#The number of training epochs that we are going to run. For each epoch we go through the entire data set.
</span><span class="n">batch_size</span> <span class="o">=</span> <span class="mi">100</span> <span class="c1">#The number of training examples that we are going to send through the RBM at a time. 
</span><span class="n">lr</span>         <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">constant</span><span class="p">(</span><span class="mf">0.005</span><span class="p">,</span> <span class="n">tf</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span> <span class="c1">#The learning rate of our model
</span>
<span class="c1">### Variables:
# Next, let's look at the variables we're going to use:
</span>
<span class="n">x</span>  <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">float32</span><span class="p">,</span> <span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="n">n_visible</span><span class="p">],</span> <span class="n">name</span><span class="o">=</span><span class="s">"x"</span><span class="p">)</span> <span class="c1">#The placeholder variable that holds our data
</span><span class="n">W</span>  <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">random_normal</span><span class="p">([</span><span class="n">n_visible</span><span class="p">,</span> <span class="n">n_hidden</span><span class="p">],</span> <span class="mf">0.01</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s">"W"</span><span class="p">)</span> <span class="c1">#The weight matrix that stores the edge weights
</span><span class="n">bh</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">zeros</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_hidden</span><span class="p">],</span>  <span class="n">tf</span><span class="p">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s">"bh"</span><span class="p">))</span> <span class="c1">#The bias vector for the hidden layer
</span><span class="n">bv</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">zeros</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_visible</span><span class="p">],</span>  <span class="n">tf</span><span class="p">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s">"bv"</span><span class="p">))</span> <span class="c1">#The bias vector for the visible layer
</span>

<span class="c1">#### Helper functions. 
</span>
<span class="c1">#This function lets us easily sample from a vector of probabilities
</span><span class="k">def</span> <span class="nf">sample</span><span class="p">(</span><span class="n">probs</span><span class="p">):</span>
    <span class="c1">#Takes in a vector of probabilities, and returns a random vector of 0s and 1s sampled from the input vector
</span>    <span class="k">return</span> <span class="n">tf</span><span class="p">.</span><span class="n">floor</span><span class="p">(</span><span class="n">probs</span> <span class="o">+</span> <span class="n">tf</span><span class="p">.</span><span class="n">random_uniform</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">shape</span><span class="p">(</span><span class="n">probs</span><span class="p">),</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

<span class="c1">#This function runs the gibbs chain. We will call this function in two places:
#    - When we define the training update step
#    - When we sample our music segments from the trained RBM
</span><span class="k">def</span> <span class="nf">gibbs_sample</span><span class="p">(</span><span class="n">k</span><span class="p">):</span>
    <span class="c1">#Runs a k-step gibbs chain to sample from the probability distribution of the RBM defined by W, bh, bv
</span>    <span class="k">def</span> <span class="nf">gibbs_step</span><span class="p">(</span><span class="n">count</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">xk</span><span class="p">):</span>
        <span class="c1">#Runs a single gibbs step. The visible values are initialized to xk
</span>        <span class="n">hk</span> <span class="o">=</span> <span class="n">sample</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">xk</span><span class="p">,</span> <span class="n">W</span><span class="p">)</span> <span class="o">+</span> <span class="n">bh</span><span class="p">))</span> <span class="c1">#Propagate the visible values to sample the hidden values
</span>        <span class="n">xk</span> <span class="o">=</span> <span class="n">sample</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">hk</span><span class="p">,</span> <span class="n">tf</span><span class="p">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">W</span><span class="p">))</span> <span class="o">+</span> <span class="n">bv</span><span class="p">))</span> <span class="c1">#Propagate the hidden values to sample the visible values
</span>        <span class="k">return</span> <span class="n">count</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">xk</span>

    <span class="c1">#Run gibbs steps for k iterations
</span>    <span class="n">ct</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">constant</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="c1">#counter
</span>    <span class="p">[</span><span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">x_sample</span><span class="p">]</span> <span class="o">=</span> <span class="n">control_flow_ops</span><span class="p">.</span><span class="n">While</span><span class="p">(</span><span class="k">lambda</span> <span class="n">count</span><span class="p">,</span> <span class="n">num_iter</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="n">count</span> <span class="o">&lt;</span> <span class="n">num_iter</span><span class="p">,</span>
                                         <span class="n">gibbs_step</span><span class="p">,</span> <span class="p">[</span><span class="n">ct</span><span class="p">,</span> <span class="n">tf</span><span class="p">.</span><span class="n">constant</span><span class="p">(</span><span class="n">k</span><span class="p">),</span> <span class="n">x</span><span class="p">],</span> <span class="mi">1</span><span class="p">,</span> <span class="bp">False</span><span class="p">)</span>
    <span class="c1">#This is not strictly necessary in this implementation, but if you want to adapt this code to use one of TensorFlow's
</span>    <span class="c1">#optimizers, you need this in order to stop tensorflow from propagating gradients back through the gibbs step
</span>    <span class="n">x_sample</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">stop_gradient</span><span class="p">(</span><span class="n">x_sample</span><span class="p">)</span> 
    <span class="k">return</span> <span class="n">x_sample</span>

<span class="c1">### Training Update Code
# Now we implement the contrastive divergence algorithm. First, we get the samples of x and h from the probability distribution
#The sample of x
</span><span class="n">x_sample</span> <span class="o">=</span> <span class="n">gibbs_sample</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> 
<span class="c1">#The sample of the hidden nodes, starting from the visible state of x
</span><span class="n">h</span> <span class="o">=</span> <span class="n">sample</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">W</span><span class="p">)</span> <span class="o">+</span> <span class="n">bh</span><span class="p">))</span> 
<span class="c1">#The sample of the hidden nodes, starting from the visible state of x_sample
</span><span class="n">h_sample</span> <span class="o">=</span> <span class="n">sample</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">x_sample</span><span class="p">,</span> <span class="n">W</span><span class="p">)</span> <span class="o">+</span> <span class="n">bh</span><span class="p">))</span> 

<span class="c1">#Next, we update the values of W, bh, and bv, based on the difference between the samples that we drew and the original values
</span><span class="n">size_bt</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">cast</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">shape</span><span class="p">(</span><span class="n">x</span><span class="p">)[</span><span class="mi">0</span><span class="p">],</span> <span class="n">tf</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">W_adder</span>  <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">mul</span><span class="p">(</span><span class="n">lr</span><span class="o">/</span><span class="n">size_bt</span><span class="p">,</span> <span class="n">tf</span><span class="p">.</span><span class="n">sub</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">h</span><span class="p">),</span> <span class="n">tf</span><span class="p">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">x_sample</span><span class="p">),</span> <span class="n">h_sample</span><span class="p">)))</span>
<span class="n">bv_adder</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">mul</span><span class="p">(</span><span class="n">lr</span><span class="o">/</span><span class="n">size_bt</span><span class="p">,</span> <span class="n">tf</span><span class="p">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">sub</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x_sample</span><span class="p">),</span> <span class="mi">0</span><span class="p">,</span> <span class="bp">True</span><span class="p">))</span>
<span class="n">bh_adder</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">mul</span><span class="p">(</span><span class="n">lr</span><span class="o">/</span><span class="n">size_bt</span><span class="p">,</span> <span class="n">tf</span><span class="p">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">sub</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">h_sample</span><span class="p">),</span> <span class="mi">0</span><span class="p">,</span> <span class="bp">True</span><span class="p">))</span>
<span class="c1">#When we do sess.run(updt), TensorFlow will run all 3 update steps
</span><span class="n">updt</span> <span class="o">=</span> <span class="p">[</span><span class="n">W</span><span class="p">.</span><span class="n">assign_add</span><span class="p">(</span><span class="n">W_adder</span><span class="p">),</span> <span class="n">bv</span><span class="p">.</span><span class="n">assign_add</span><span class="p">(</span><span class="n">bv_adder</span><span class="p">),</span> <span class="n">bh</span><span class="p">.</span><span class="n">assign_add</span><span class="p">(</span><span class="n">bh_adder</span><span class="p">)]</span>


<span class="c1">### Run the graph!
# Now it's time to start a session and run the graph! 
</span>
<span class="k">with</span> <span class="n">tf</span><span class="p">.</span><span class="n">Session</span><span class="p">()</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
    <span class="c1">#First, we train the model
</span>    <span class="c1">#initialize the variables of the model
</span>    <span class="n">init</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">initialize_all_variables</span><span class="p">()</span>
    <span class="n">sess</span><span class="p">.</span><span class="n">run</span><span class="p">(</span><span class="n">init</span><span class="p">)</span>
    <span class="c1">#Run through all of the training data num_epochs times
</span>    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">)):</span>
        <span class="k">for</span> <span class="n">song</span> <span class="ow">in</span> <span class="n">songs</span><span class="p">:</span>
            <span class="c1">#The songs are stored in a time x notes format. The size of each song is timesteps_in_song x 2*note_range
</span>            <span class="c1">#Here we reshape the songs so that each training example is a vector with num_timesteps x 2*note_range elements
</span>            <span class="n">song</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="n">song</span><span class="p">)</span>
            <span class="n">song</span> <span class="o">=</span> <span class="n">song</span><span class="p">[:</span><span class="n">np</span><span class="p">.</span><span class="n">floor</span><span class="p">(</span><span class="n">song</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">/</span><span class="n">num_timesteps</span><span class="p">)</span><span class="o">*</span><span class="n">num_timesteps</span><span class="p">]</span>
            <span class="n">song</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">song</span><span class="p">,</span> <span class="p">[</span><span class="n">song</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">/</span><span class="n">num_timesteps</span><span class="p">,</span> <span class="n">song</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="n">num_timesteps</span><span class="p">])</span>
            <span class="c1">#Train the RBM on batch_size examples at a time
</span>            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">song</span><span class="p">),</span> <span class="n">batch_size</span><span class="p">):</span> 
                <span class="n">tr_x</span> <span class="o">=</span> <span class="n">song</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="n">batch_size</span><span class="p">]</span>
                <span class="n">sess</span><span class="p">.</span><span class="n">run</span><span class="p">(</span><span class="n">updt</span><span class="p">,</span> <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">x</span><span class="p">:</span> <span class="n">tr_x</span><span class="p">})</span>

    <span class="c1">#Now the model is fully trained, so let's make some music! 
</span>    <span class="c1">#Run a gibbs chain where the visible nodes are initialized to 0
</span>    <span class="n">sample</span> <span class="o">=</span> <span class="n">gibbs_sample</span><span class="p">(</span><span class="mi">1</span><span class="p">).</span><span class="nb">eval</span><span class="p">(</span><span class="n">session</span><span class="o">=</span><span class="n">sess</span><span class="p">,</span> <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">x</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">10</span><span class="p">,</span> <span class="n">n_visible</span><span class="p">))})</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">sample</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">any</span><span class="p">(</span><span class="n">sample</span><span class="p">[</span><span class="n">i</span><span class="p">,:]):</span>
            <span class="k">continue</span>
        <span class="c1">#Here we reshape the vector to be time x notes, and then save the vector as a midi file
</span>        <span class="n">S</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">sample</span><span class="p">[</span><span class="n">i</span><span class="p">,:],</span> <span class="p">(</span><span class="n">num_timesteps</span><span class="p">,</span> <span class="mi">2</span><span class="o">*</span><span class="n">note_range</span><span class="p">))</span>
        <span class="n">midi_manipulation</span><span class="p">.</span><span class="n">noteStateMatrixToMidi</span><span class="p">(</span><span class="n">S</span><span class="p">,</span> <span class="s">"generated_chord_{}"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">i</span><span class="p">))</span>
</code></pre></div></div>

<h3 id="music-samples">Music Samples</h3>

<p>By altering the <code class="language-plaintext highlighter-rouge">num_timesteps</code> variable in the above code, we can make sequences of music of different lengths. Since each additional timestep increases the dimensionality of the vectors we are inputting to the model, we are limited to sequences of only a few seconds. In my next post, Iâ€™ll show how we can adapt this model to generate longer sequences of music.</p>

<h4 id="15-timesteps">15 timesteps</h4>
<audio src="/audio/15_1.mp3" controls="" preload=""></audio>
<audio src="/audio/15_2.mp3" controls="" preload=""></audio>

<h4 id="10-timesteps">10 timesteps</h4>
<audio src="/audio/10_1.mp3" controls="" preload=""></audio>
<audio src="/audio/10_2.mp3" controls="" preload=""></audio>
<audio src="/audio/10_3.mp3" controls="" preload=""></audio>
<audio src="/audio/10_4.mp3" controls="" preload=""></audio>
<audio src="/audio/10_5.mp3" controls="" preload=""></audio>

<h4 id="5-timesteps">5 timesteps</h4>
<audio src="/audio/5_1.mp3" controls="" preload=""></audio>
<audio src="/audio/5_2.mp3" controls="" preload=""></audio>
<audio src="/audio/5_3.mp3" controls="" preload=""></audio>

<h2 id="wrap-up">Wrap Up</h2>
<p>I hoped you enjoyed this post about RBMs in TensorFlow! RBMs have a plethora of applications that are more useful than generating short sequences of music, but their real power comes when we build them into more complex models. For example, we can form Deep Belief Networks by stacking RBMs into multiple layers. Training a DBN is simple - we greedily train each RBM, and pass the mean activations of each trained RBM as input to the next layer. Unlike RBMs, DBNs can take advantage of deep learning to learn hierarchical features of the data.</p>

<p>In the next post, weâ€™re going to talk about a different model that builds on the RBM - the RNN-RBM. This model is a sequence of RBMs, where the parameters of each RBM is determined by an Recurrent Neural Network. This architecture can model temporal dependencies, and we will use the model to generate much longer, more complex, and better-sounding musical pieces.</p>

:ET