I"·<script> 
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-82391879-1', 'auto');
  ga('send', 'pageview');

</script>

<!-- http://www.math.ucsd.edu/~fan/research/cb/ch1.pdf -->
<!-- https://www2.imm.dtu.dk/projects/manifold/Papers/Laplacian.pdf -->

<!-- Generalized EigenProblem: https://www.geo.tuwien.ac.at/downloads/tm/svd.pdf -->

<p>At first glance, PCA and Laplacian Eigenmaps seem both very similar. We can view both algorithms as constructing a graph from our data, choosing a matrix to represent this graph, computing the eigenvectors of this matrix, and then using these eigenvectors to determine low-dimensionality embeddings of our data.</p>

<p>However, the algorithms can produce very different results, primarily due to the fact that PCA is a linear dimensionality reduction algorithm that makes few assumptions, whereas Laplacian Eigenmaps is a nonlinear dimensionality reduction algorithm that assumes that the data lies on a low-dimensional manifold.</p>

<p>We can explicitly characterize this divergence in terms of three key differences between the algorithms:</p>
<ul>
  <li>The graph we construct from our data</li>
  <li>The choice of matrix representation of this graph</li>
  <li>The choice of eigenvectors of this matrix</li>
</ul>

<p>In the following exposition, we will assume we have \(n\) data points and \(d\) features, represented in a data matrix \(X\) with shape \(n \times d\). For simplicity we will assume that our data is already normalized and zero-centered.</p>

<h2 id="pca">PCA</h2>
<p>In PCA, we form a fully connected graph such that the edge between each pair of vertices has weight equal to the dot product of the corresponding data vectors. We represent this graph with the adjacency matrix, \(X^T X\). For some \(d' &lt; d\) our objective is then to construct the \(n \times d'\) matrix \(X'\) such that the following quantity is minimized.</p>

\[\sum_{i,j} (X'^T X' - X^T X)_{ij}^2\]

<p>In order to do this, we compute the eigenvectors \(v\) corresponding to the \(d'\) largest eigenvalues \(\lambda\) of the \(n \times n\) matrix \(X^T X\). In practice we implement this with SVD rather than explicitly form the matrix \(X^T X\). We then define \(X'_{i,j}\) to be \(v_{j_i}*\lambda_{j}^{1/2}\). That is, the jth element of the ith data pointâ€™s embedding is the product of the square root of the jth largest eigenvalue and the ith element of the corresponding eigenvector.</p>

<p>Because PCA operates over the fully connected graph, all pairwise relationships between data vectors (i.e. elements of \(X^T X\)) are considered equally important, and the optimization objective is framed in terms of reconstructing the exact distances between points.</p>

<h2 id="laplacian-eigenmaps">Laplacian Eigenmaps</h2>
<p>In contrast, in Laplacian Eigenmaps we form a different graph based on a nonlinear transformation of our data, and we represent this graph with its Laplacian matrix.</p>

<p>We begin by choosing a number \(k\) and building a graph such that there is a unit-weight edge connecting the vertices \(v_i\) and \(v_j\) if \(v_i\) is one of the kth nearest neighbors of \(v_j\) or vice-versa. In some implementations of Laplacian Eigenmaps, the weight of this edge is defined to be inversely proportional to the distance between the data vectors corresponding to \(v_i\) and \(v_j\).</p>

<p>Now for some \(d' &lt; d\) our objective is then to construct the \(n \times d'\) matrix \(X'\) such that the following quantity is minimized subject to a few constraints around orthogonality and embedding normalization.</p>

\[\sum_{i,j \ | \ i \sim j} (x'_i - x'_j)^2\]

<p>Note that \(x'_i\) is the ith row of \(X'\) and that \(i \sim j\) if there is an edge between \(v_i\) and \(v_j\):</p>

<p>In order to do this, we compute the eigenvectors \(v\) corresponding to the \(d'\) smallest eigenvalues for the generalized eigenproblem \(Ly = \lambda Dy\), where \(D\) is the \(n \times n\) diagonal matrix where the ith diagonal entry is the degree of \(v_i\). Note that this is equivalent to computing the eigenvectors of the matrix \(D^{-1}L\). We then define \(X'_{i,j}\) to be \(v_{j_i}\). That is, the jth element of the ith data pointâ€™s embedding is the ith element of the eigenvector corresponding to the jth smallest eigenvalue.</p>

<p>Unlike PCA, the Laplacian Eigenmaps algorithm does not try to preserve exact pairwise distances, or even relative pairwise distances between far away points. The algorithm exclusively focuses on mapping the embeddings corresponding to nearest neighbors as close together as possible.</p>

<h2 id="summary">Summary</h2>

<h5 id="graph">Graph</h5>
<ul>
  <li><em>PCA</em>: Fully connected graph where weights of the graph are determined by the dot product between data points</li>
  <li><em>Laplacian Eigenmaps</em>: Graph where an edge only exists between \(v_i\) and \(v_j\) if \(v_i\) is one of the kth nearest neighbors of \(v_j\) or vice-versa</li>
</ul>

<h5 id="matrix">Matrix</h5>
<ul>
  <li><em>PCA</em>: The adjacency matrix</li>
  <li><em>Laplacian Eigenmaps</em>: The Laplacian matrix</li>
</ul>

<h5 id="eigenvectors">Eigenvectors</h5>
<ul>
  <li><em>PCA</em>: The eigenvectors corresponding to the \(d'\) largest eigenvalues</li>
  <li><em>Laplacian Eigenmaps</em>: The eigenvectors corresponding to the \(d'\) smallest eigenvalues</li>
</ul>

<!-- 

















Generalized eigenvectors/eigenvalues minimize the Rayleigh quotient, which is large when points that are connected in the graph are mapped away from each other

The eigenvectors of the




the vertices  each data vector pair of neighbors 



aim to minimize the squared reconstruction error of the pairwise covariances. 







However, PCA is a linear dimensionality reduction algorithm that essentially aims to directly minimize the difference between the squared 



aims to preserve the maximal variance 

(if we)

t such that the following quantity is minimized, where $$i \sim j$$ if there is an edge between $$v_i$$ and $$v_j$$ and $$d_{v_i}$$ is the degree of $$v_i$$:

$$
\frac{
  \sum_{i,j \|\ i \sim j} (x'_1 - x'_0)^2
}{
  \sum_{i} (x'_i)^2 d_{v_i}
}
$$





are implicitly compute the eigenvectors of a matrix 

(in )


$$\{0,1\}$$ or $$\mathbb{R}$$.


In PCA, we project data points 


 -->
:ET