---
layout: post
title: Optimizers as Dynamical Systems
tags: [Machine Learning, Category Theory, Lens, Dynamical System, Gradient Descent]
---
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-82391879-1', 'auto');
  ga('send', 'pageview');

</script>




Consider a system for forecasting a time series in $$\mathbb{R}$$. At each time $$t$$ this system will use the state of the world (represented as a vector in $$\mathbb{R}^a$$) to predict what the value of the time series (a real number) will be at time $$t+1$$. At time $$t+1$$ the system will receive information about the correct value of the time series at time $$t+1$$, represented as a pair $$(x_a, y) \in \mathbb{R}^a \times \mathbb{R}$$, and will need to predict the value of the time series at time $$t+2$$.

One way to build such a system would be to choose a loss function $$l: \mathbb{R} \times \mathbb{R} \rightarrow \mathbb{R}$$ and use gradient descent to train a model $$f: \mathbb{R}^p \times \mathbb{R}^a \rightarrow \mathbb{R}$$ on all of the data before some time $$t$$. We could then use that trained model to generate predictions at $$t+n$$. If the time series is not stationary then this may produce poor results for $$t+n$$ where $$n$$ is large.

Another option would be to continuously update the parameters $$x_p \in \mathbb{R}^p$$ of the model $$f: \mathbb{R}^p \times \mathbb{R}^a \rightarrow \mathbb{R}$$ as each new sample $$(x_a, y) \in \mathbb{R}^a \times \mathbb{R}$$ is observed. This is known as online learning.

An example online learning algorithm is stochastic gradient descent, which we can define as follows
<!--
, and we can use algorithms like stochastic gradient descent (Definition \ref{definition:stochastic-gradient-descent}) or stochastic momentum (Definition \ref{definition:stochastic-momentum}) to apply this strategy. We can represent this with the following dynamical systems:


Given some set:

$$
S = \{(x_{a_i}, y_{i}) \ |\ x_{a_i} \in \mathbb{R}^a, y_{i} \in \mathbb{R}\}
$$

and functions:

$$
l: \mathbb{R} \times \mathbb{R} \rightarrow \mathbb{R}
\qquad
f: \mathbb{R}^p \times \mathbb{R}^a \rightarrow \mathbb{R}^b
$$

suppose we want to minimize the function:

$$
l_f(x_p) = \sum_{(x_{a_i}, y_{i}) \in S} l(f(x_p, x_{a_i}), y_{i})
$$

If $$S$$ is large then this might be difficult to compute directly. One way that we can optimize this function without computing it directly is to apply the following algorithm, which we call stochastic gradient descent: -->

$$
\text{Initialize $$x_p$$ randomly} \\
\text{As each sample $$(x_{a_i}, y_{i})$$ arrives:}\\
i \gets mod(i+1, |S|)\\
l_{f_i} \gets l(f(x_p, x_{a_i}), y_{i})\\
x_p \gets x_p - \alpha \nabla l_{f_i}(x_p)
$$


Stochastic gradient descent describes a dynamical system in which a state in $$\mathbb{R}^p$$ evolves over time in response to inputs in $$\mathbb{R}^a \times \mathbb{R}$$. One of the downsides of stochastic gradient descent is that the update step can be very high variance from $$t$$ to $$t+1$$, which can slow down convergence. One way to get around this is to use the momentum, which we define as follows:

$$
\text{Initialize $$x_p, x'_p$$ randomly} \\
\text{As each sample $$(x_{a_i}, y_{i})$$ arrives:}\\
i \gets mod(i+1, |S|)\\
l_{f_i} \gets l(f(x_p, x_{a_i}), y_{i})\\
x_p \gets x_p + \alpha x'_p \\
x'_p \gets (1 - \beta) x'_p - \beta \nabla l(x_p) \\
$$


Momentum describes a dynamical system in which a state in $$\mathbb{R}^p \times \mathbb{R}^p$$ evolves over time in response to inputs in $$\mathbb{R}^a \times \mathbb{R}$$.






### Lenses and Dynamical Systems



Optimization algorithms like stochastic gradient descent and momentum describe a dynamical system whose state is the function parameters. In this section we dig deeper into this perspective. In particular, we use [David Jaz Myer's category theoretic formulation of dynamical systems](http://davidjaz.com/Papers/DynamicalBook.pdf) to study how we can recombine simpler optimization algorithms to form a more complex algorithm.

The category theoretic formulation of dynamical systems is based on lenses, which are a tool for representing certain kinds of compositions. We will focus entirely on lenses in the category of sets and functions.

A lens in the category $$\mathbf{Set}$$ of sets and functions

$$
\left(_{A}^{A'}\right)
\xrightarrow{(f_g, f_p)}
\left(_{B}^{B'}\right)
$$

is be a pair $$(f_g, f_p)$$ of morphisms (functions):

$$
    f_g : A \rightarrow B
    \qquad
    f_p : A \times B' \rightarrow A'
$$


Lenses are powerful because many computations can be expressed in terms of the combination of multiple lenses. The simplest way to combine lenses is to stack them in parallel.

Given the lenses:

$$
    \left(_{A}^{A'}\right)
    \xrightarrow{(f_g, f_p)}
    \left(_{B}^{B'}\right)
    \\
    \left(_{C}^{C'}\right)
    \xrightarrow{(g_g, g_p)}
    \left(_{D}^{D'}\right)
$$

we define their monoidal product to be the lens:

$$
    \left(_{A \otimes C}^{A' \otimes C'} \right) \xrightarrow{(h_g, h_p)}
    \left(_{B \otimes D}^{B' \otimes D'}\right)
$$

where:

$$
    h_g = f_g \otimes g_g
    \\
    h_p =
    \langle f_p \circ (\pi_0 \otimes \pi_0)
    ,
    (g_p \circ (\pi_1 \otimes \pi_1)\rangle
$$

<!-- which we can equivalently write as:
\begin{figure}[H]
  \centering
  \[ \input{figures/lens-tensor.tikz} \]
  \label{fig:lens-tensor}
  \caption{Monoidal product of lenses}
\end{figure}
\end{definition} -->

We can also compose lenses directly. Given the lenses:

$$
    \left(_{A}^{A'}\right)
    \xrightarrow{(f_g, f_p)}
    \left(_{B}^{B'}\right)
    \qquad
    \left(_{B}^{B'}\right)
    \xrightarrow{(g_g, g_p)}
    \left(_{C}^{C'}\right)
$$

we define their composition to be the lens:

$$
\left(_{A}^{A'}\right)
\xrightarrow{(h_g, h_p)}
\left(_{C}^{C'}\right)
$$

where:

$$
    h_g = g_g \circ f_g
    \\
    h_p =
    f_p \circ \langle \pi_0, (g_p \circ ((f_g\circ \pi_0) \otimes \pi_1))\rangle
$$

We can now characterize dynamical systems as lenses.

A discrete system is a lens:
$$
\lenssig{S}{S}{O}{I}{f_g}{f_p}
$$

or equivalently, a set of states $$S$$, a set of inputs $$I$$, a set of outputs $$O$$, and two functions:

* A get (read) function $$f_g: S \rightarrow O$$ that generates an output from a state.
* A put (update) function $$f_p: S \times I \rightarrow S$$ that takes a pair of a state and an input and returns an updated state.


Intuitively, a discrete system represents the stepwise application of the update function $$f_p$$, potentially in response to a sequence of inputs. That is, the discrete system:

$$
\left(_{S}^{S}\right)
\xrightarrow{(f_g, f_p)}
\left(_{O}^{I}\right)
$$

describes a dynamical system whose state  $$x_{S_t} \in S$$ at time $$t+1$$ is described by the equation:

$$
     x_{S_{t+1}} = x_{S_t} + f_p(x_{S_t}, x_{I_t})
$$

where $$x_{I_t} \in I$$ is the system input at time $$t$$.













### Constructing Momentum from Stochastic Gradient Descent


Given a pair of a loss function $$l: \mathbb{R} \times \mathbb{R} \rightarrow \mathbb{R}$$ and inference function $$f: \mathbb{R}^p \times \mathbb{R}^a \rightarrow \mathbb{R}$$ the stochastic gradient descent dynamical system $$sg$$ has the following structure:
$$
\left(_{\mathbb{R}^{p}}^{\mathbb{R}^{p}}\right)
\xrightarrow{(sg_g, sg_p)}
\left(_{\mathbb{R}^p}^{\mathbb{R}^a \times \mathbb{R}}\right)
\\
    sg_g(x_p) = x_p
    \\
    sg_p(x_p, (x_a, y)) = -\nabla l(f(x_p, x_a), y)
$$

The open gradient descent system iteratively updates the parameters $$x_p$$ each time a new sample $$(x_a, y) \in \mathbb{R}^a \times \mathbb{R}$$ is observed.

We can also define a dynamical system to represent stochastic momentum:

Given a pair of a loss function $$l: \mathbb{R} \times \mathbb{R} \rightarrow \mathbb{R}$$ and inference function $$f: \mathbb{R}^p \times \mathbb{R}^a \rightarrow \mathbb{R}$$ the momentum dynamical system $$sm$$ has the following structure:

$$
\left(_{\mathbb{R}^{p} \times \mathbb{R}^{p}}^{\mathbb{R}^p \times \mathbb{R}^{p}}\right)
\xrightarrow{(sm_g, sm_p)}
\left(_{\mathbb{R}^p}^{\mathbb{R}^a \times \mathbb{R}}\right)
\\
    sm_g((x_p, x'_p)) = x_p
    \\
    sm_p((x_p, x'_p), (x_a, y)) = (x'_p, - x'_p -\nabla l(f(x_p, x_a), y))
$$

We can construct momentum from the composition and tensor of stochastic gradient descent with some basic lenses. To start, consider the following discrete systems.

* The discrete system $$add$$ reads the state directly and uses the sum of the input values as the state update:
$$
\left(_{\mathbb{R}^{p}}^{\mathbb{R}^{p}}\right)
\xrightarrow{(add_g, add_p)}
\left(_{\mathbb{R}^p}^{\mathbb{R}^p \times \mathbb{R}^p}\right)
\\
add_g(x_p) = x_p
\\
add_p(x_p, (x'_p,x''_p)) = x'_p+x''_p
$$

* The discrete system $$cp$$ reads the state directly and uses the current state as the state update:
$$
\left(_{\mathbb{R}^{p}}^{\mathbb{R}^{p}}\right)
\xrightarrow{(cp_g, cp_p)}
\left(_{\mathbb{R}^p}^{*}\right)
\\
cp_g(x_p) = x_p
\\
cp_p(x_p, *) = x_p
$$

* The discrete system $$sw$$ reads the state directly and swaps the positions of the input values to generate the state update:
$$
\left(_{\mathbb{R}^p \times \mathbb{R}^p}^{\mathbb{R}^p \times \mathbb{R}^p}\right)
\xrightarrow{(sw_g, sw_p)}
\left(_{\mathbb{R}^p \times \mathbb{R}^p}^{\mathbb{R}^p \times \mathbb{R}^p}\right)
\\
sw_g(x_p, x'_p) = (x_p, x'_p)
\\
sw_p((x_p, x'_p), (x''_p, x'''_p)) = (x'''_p, x''_p)
$$

Consider also the following lenses:
* The lens $$ng$$ uses the negated input value as the state update:
$$
\left(_{*}^{\mathbb{R}^{p}}\right)
\xrightarrow{(ng_g, ng_p)}
\left(_{*}^{\mathbb{R}^p}\right)
\\
ng_g(*) = *
\\
ng_p(*, x_p) = -x_p
$$

* The lens $$srp$$ reads the left component of the system state and uses the right component to generate the state update:
$$
\left(_{\mathbb{R}^p \times \mathbb{R}^p}^{\mathbb{R}^a \times \mathbb{R} \times \mathbb{R}^{p}}\right)
\xrightarrow{(srp_g, srp_p)}
\left(_{\mathbb{R}^p}^{\mathbb{R}^a \times \mathbb{R}}\right)
\\
srp_g(x_p, x'_p) = x_p
\\
srp_p((x_p, x'_p), (x_a, y)) = (x_a, y, x'_p)
$$


We can now construct the closed momentum dynamical system $$m$$ as the following composition:
$$
    sm = srp \circ (((sg \times ng) \circ add) \times cp) \circ sw
$$










Let's break this down into its component parts to see this more clearly. We can draw the composition:

$$
\left(_{\mathbb{R}^{p}}^{\mathbb{R}^{p}}\right)
\xrightarrow{
  (((sg \times ng) \circ add)_g,
  ((sg \times ng) \circ add)_p)}
\left(_{\mathbb{R}^p}^{\mathbb{R}^a \times \mathbb{R} \times \mathbb{R}^{p}}\right)
$$
where
$$
((sg \times ng) \circ add)_g(x_p) =
(sg \times ng)_g(add_g(x_p)) =
(sg \times ng)_g(x_p) =
x_p
$$
and:
$$
    ((sg \times ng) \circ add)_p(x_p, (x_a, y, c_p)) = \\
    add_p(x_p, (sg \times ng)_p(add_g(x_p), (x_a, y, c_p))) = \\
    add_p(x_p, sg_p(add_g(x_p), (x_a, y)), ng_p(c_p)) = \\
    add_p(x_p, sg_p(x_p, (x_a, y)), -c_p) = \\
    add_p(x_p, -\nabla l(f(x_p, x_a), y), -c_p) = \\
    -c_p - \nabla l(f(x_p, x_a), y)
$$
<!--
We can also draw this follows:

\begin{figure}[H]
  \centering
  \[ \input{figures/sg_ng_add.tikz} \]
  \label{fig:sg_ng_add}
  \caption{$$((sg \times ng) \circ add)_p: \mathbb{R}^p \times \mathbb{R}^a \times \mathbb{R} \times \mathbb{R}^p \rightarrow \mathbb{R}^p$$}
\end{figure} -->

We can build on this to form:

$$
\left(_{\mathbb{R}^{p} \times \mathbb{R}^{p}}^
       {\mathbb{R}^p \times \mathbb{R}^{p}}\right)
\xrightarrow{
  ((((sg \times ng) \circ add)\times cp)_g,
   (((sg \times ng) \circ add)\times cp)_p)
}
\left(_{\mathbb{R}^p \times \mathbb{R}^p}^
       {\mathbb{R}^a \times \mathbb{R} \times \mathbb{R}^p}\right)
$$

where:

$$
    (((g \times ng) \circ add)\times cp)_g(x_p, x'_p) = (x_p,x'_p)
    \\
    (((g \times ng) \circ add)\times cp)_p((x_p, x'_p), (x_a, y, c_p))) =
    (-c_p - \nabla l(f(x_p, x_a), y), x'_p)
$$

We can further build on this to form:

$$
\left(_{\mathbb{R}^{p} \times \mathbb{R}^{p}}^
       {\mathbb{R}^p \times \mathbb{R}^{p}}\right)
\xrightarrow{
  (((((sg \times ng) \circ add)\times cp) \circ sw)_g,
   ((((sg \times ng) \circ add)\times cp) \circ sw)_p)
}
\left(_{\mathbb{R}^p \times \mathbb{R}^p}^
       {\mathbb{R}^a \times \mathbb{R} \times \mathbb{R}^p}\right)
$$

where:

$$
    ((((sg \times ng) \circ add)\times cp) \circ sw)_g(x_p, x'_p) = (x_p,x'_p)
    \\
    ((((sg \times ng) \circ add)\times cp) \circ sw)_p((x_p, x'_p), (x_a, y, c_p))) =
    (x'_p, -c_p - \nabla l(f(x_p, x_a), y))
$$


Putting it all together we have:

$$
\left(_{\mathbb{R}^{p} \times \mathbb{R}^{p}}^
       {\mathbb{R}^p \times \mathbb{R}^{p}}\right)
\xrightarrow{
  ((srp \circ ((((sg \times ng) \circ add)\times cp) \circ sw))_g,
   (srp \circ ((((sg \times ng) \circ add)\times cp) \circ sw))_p)
}
\left(_{\mathbb{R}^p}^
       {\mathbb{R}^a \times \mathbb{R}}\right)
$$
where:
$$
    (srp \circ ((((sg \times ng) \circ add)\times cp) \circ sw))_g(x_p, x'_p) =
    x_p =
    sm_g((x_p, x'_p))
$$

and we have:

$$
    (srp \circ ((((sg \times ng) \circ add)\times cp) \circ sw))_p((x_p, x'_p), (x_a, y)) = \\

    (x'_p, -x'_p - \nabla l(f(x_p, x_a), y)) = \\

    sm_p((x_p, x'_p), (x_a, y))
$$

### Conclusions

In this post we explored how we can leverage the composition of dynamical systems to construct complex optimization algorithms from simpler components. In particular, we demonstrated that the momentum optimization algorithms can be constructed from nothing more than stochastic gradient descent and some simple lens operations. It should be simple to extend this strategy to other algorithms like Adagrad and Adam.  

Furthermore, we can probably utilize this dynamical systems perspective to reason about the relationship between the optimization process and the data that we feed in from the outside. We can similarly represent the optimization hyperparameters or the configuration of the data generation process as the dynamical system input.












<!-- Note that we can equivalently write $$h_p$$ as:

\begin{figure}[H]
  \centering
  \[ \input{figures/lens-composition-put.tikz} \]
  \label{fig:lens-composition}
  \caption{Composition of lenses}
\end{figure}
\end{definition} -->





<!--
We will primarily work with a special concrete case of optics, which we call lenses \citep{CategoriesOfOptics, clarke2020profunctor, fosterlenses, OpenDiagrams, LensesHedges}.

\begin{definition}\label{definition:lens}
[Def. 1 in \citep{LensesHedges}]

\end{definition}

Lenses are powerful because many computations can be expressed in terms of the combination of multiple lenses. The simplest way to combine lenses is to stack them in parallel:

\begin{definition}\label{definition:lens-monoidal-product}
[Def. 1 in \citep{LensesHedges}]
Given the lenses:

$$
    \lenssig
    {A}{A'}
    {B}{B'}
    {f_g}{f_p}
    \\
    \lenssig
    {C}{C'}
    {D}{D'}
    {g_g}{g_p}
$$

we define their monoidal product to be the lens:

$$
    \lenssig
    {A \otimes C}{A' \otimes C'}
    {B \otimes D}{B' \otimes D'}
    {h_g}{h_p}
$$

where:

$$
    h_g = f_g \otimes g_g
    \\
    h_p =
    \langle f_p \circ (\pi_0 \otimes \pi_0)
    ,
    (g_p \circ (\pi_1 \otimes \pi_1)\rangle
$$

which we can equivalently write as:
\begin{figure}[H]
  \centering
  \[ \input{figures/lens-tensor.tikz} \]
  \label{fig:lens-tensor}
  \caption{Monoidal product of lenses}
\end{figure}
\end{definition}

We can also compose lenses directly:

\begin{definition}\label{definition:lens-composition}
Given the lenses:

$$
    \lenssig
    {A}{A'}
    {B}{B'}
    {f_g}{f_p}
    \qquad
    \lenssig
    {B}{B'}
    {C}{C'}
    {g_g}{g_p}
$$

we define their composition to be the lens:

$$
    \lenssig
    {A}{A'}
    {C}{C'}
    {h_g}{h_p}
$$

where:

$$
    h_g = g_g \circ f_g
    \\
    h_p =
    f_p \circ \langle \pi_0, (g_p \circ ((f_g\circ \pi_0) \otimes \pi_1))\rangle
$$

Note that we can equivalently write $$h_p$$ as:

\begin{figure}[H]
  \centering
  \[ \input{figures/lens-composition-put.tikz} \]
  \label{fig:lens-composition}
  \caption{Composition of lenses}
\end{figure}
\end{definition}



\cite{jazmyers2021} introduce the following characterization of dynamical systems as lenses:

\begin{definition}\label{definition:discret-system}
A discrete system is a $$\set$$-lens (Definition \ref{definition:lens}):

$$
     \lenssig{S}{S}{O}{I}{f_g}{f_p}
$$

or equivalently, a set of states $$S$$, a set of inputs $$I$$, a set of outputs $$O$$, and two functions:

\begin{itemize}
    \item A get (read) function $$f_g: S \rightarrow O$$ that generates an output from a state.

    \item A put (update) function $$f_p: S \times I \rightarrow S$$ that takes a pair of a state and an input and returns an updated state.
\end{itemize}

\end{definition}

Intuitively, a discrete system represents the stepwise application of the update function $$f_p$$, potentially in response to a sequence of inputs. That is, the discrete system:

$$
     \lenssig{S}{S}{O}{I}{f_g}{f_p}
$$

describes a dynamical system whose state  $$x_{S_t} \in S$$ at time $$t+1$$ is described by the equation:

$$
     x_{S_{t+1}} = x_{S_t} + f_p(x_{S_t}, x_{I_t})
$$

where $$x_{I_t} \in I$$ is the system input at time $$t$$.



## Constructing Open Momentum from Open Gradient Descent

In many applications we will not have a fixed dataset to optimize over, and will instead receive data as a stream. For example, consider a system for forecasting a time series in $$\mathbb{R}$$. At each time $$t$$ this system will use the state of the world (represented as a vector in $$\mathbb{R}^a$$) to predict what the value of the time series (a real number) will be at time $$t+1$$. At time $$t+1$$ the system will receive information about the correct value of the time series at time $$t+1$$, represented as a pair $$(x_a, y) \in \mathbb{R}^a \times \mathbb{R}$$, and will need to predict the value of the time series at time $$t+2$$.

One way to build such a system would be to choose a loss function $$l: \mathbb{R} \times \mathbb{R} \rightarrow \mathbb{R}$$ and use gradient descent (Definition \ref{definition:gradient-descent}) or momentum (Definition \ref{definition:momentum}) to train a model $$f: \mathbb{R}^p \times \mathbb{R}^a \rightarrow \mathbb{R}$$ on all of the data before some time $$t$$. We could then use that trained model to generate predictions at $$t+n$$. If the time series is not stationary then this may produce poor results for $$t+n$$ where $$n$$ is large. Another option would be to continuously train the model $$f$$ as each new sample $$(x_a, y) \in \mathbb{R}^a \times \mathbb{R}$$ is observed. This is known as online learning \citep{convexopt}, and we can use algorithms like stochastic gradient descent (Definition \ref{definition:stochastic-gradient-descent}) or stochastic momentum (Definition \ref{definition:stochastic-momentum}) to apply this strategy. We can represent this with the following dynamical systems:

\begin{definition}\label{definition:open-gradient-descent}
Given a pair of a loss function $$l: \mathbb{R} \times \mathbb{R} \rightarrow \mathbb{R}$$ and inference function $$f: \mathbb{R}^p \times \mathbb{R}^a \rightarrow \mathbb{R}$$ the open gradient descent dynamical system $$sg$$ has the following structure:
$$
    \lenssig
    {\mathbb{R}^{p}}
    {\mathbb{R}^p}
    {\mathbb{R}^p}
    {\mathbb{R}^a \times \mathbb{R}}
    {sg_g}{sg_p}
$$
$$
    sg_g(x_p) = x_p
    \\
    sg_p(x_p, (x_a, y)) = -\nabla l(f(x_p, x_a), y)
$$
\end{definition}

Unlike closed gradient descent, the open gradient descent system accepts an input in $$\mathbb{R}^a \times \mathbb{R}$$. We can think of the open gradient descent system as iteratively updating the parameters $$x_p$$ each time a new sample $$(x_a, y) \in \mathbb{R}^a \times \mathbb{R}$$ is observed.

We can also define a dynamical system to represent stochastic momentum:

\begin{definition}\label{definition:closed-momentum}
Given a pair of a loss function $$l: \mathbb{R} \times \mathbb{R} \rightarrow \mathbb{R}$$ and inference function $$f: \mathbb{R}^p \times \mathbb{R}^a \rightarrow \mathbb{R}$$ the open momentum dynamical system $$sm$$ has the following structure:

$$
    \lenssig
    {\mathbb{R}^{p} \times \mathbb{R}^{p}}
    {\mathbb{R}^p \times \mathbb{R}^{p}}
    {\mathbb{R}^p}
    {\mathbb{R}^a \times \mathbb{R}}
    {sm_g}
    {sm_p}
$$
$$
    sm_g((x_p, x'_p)) = x_p
    \\
    sm_p((x_p, x'_p), (x_a, y)) = (x'_p, - x'_p -\nabla l(f(x_p, x_a), y))
$$
\end{definition}

Unlike closed momentum, the open momentum system accepts an input in $$\mathbb{R}^a \times \mathbb{R}$$.

Like in Section \ref{definition:constructing-closed-momentum}, we can construct closed momentum from the composition and tensor of closed gradient descent with some basic lenses. To start, consider the following lens:

\begin{definition}
The lens $$srp$$ has the following structure:

$$
    \lenssig
    {\mathbb{R}^p \times \mathbb{R}^p}
    {\mathbb{R}^a \times \mathbb{R} \times \mathbb{R}^{p}}
    {\mathbb{R}^p}
    {\mathbb{R}^a \times \mathbb{R}}
    {srp_g}{srp_p}
$$
$$
    srp_g(x_p, x'_p) = x_p
    \\
    srp_p((x_p, x'_p), (x_a, y)) = (x_a, y, x'_p)
$$
\end{definition}

Intuitively, $$srp$$ behaves like $$rp$$ (Definition \ref{definition:rp}), except it persists the input in the update step.

We can now construct the open momentum dynamical system as the following composition:

$$
    sm = (srp \circ ((((sg \times ng) \circ add)\times cp) \circ sw))
$$

Let's break this down into its component parts to see this more clearly. We can draw the composition:

$$
    \lenssig
    {\mathbb{R}^{p}}
    {\mathbb{R}^{p}\times \mathbb{R}^{p}}
    {\mathbb{R}^p}
    {\mathbb{R}^a \times \mathbb{R} \times \mathbb{R}^{p}}
    {(sg \times ng)_g}
    {(sg \times ng)_p}
$$

and then:

$$
    \lenssig
    {\mathbb{R}^{p}}
    {\mathbb{R}^{p}}
    {\mathbb{R}^p}
    {\mathbb{R}^a \times \mathbb{R} \times \mathbb{R}^{p}}
    {((sg \times ng) \circ add)_g}
    {((sg \times ng) \circ add)_p}
$$

where:

$$
    ((sg \times ng) \circ add)_g(x_p) =
    (sg \times ng)_g(add_g(x_p)) =
    (sg \times ng)_g(x_p) =
    x_p
$$
and:
$$
    ((sg \times ng) \circ add)_p(x_p, (x_a, y, c_p)) = \\
    add_p(x_p, (sg \times ng)_p(add_g(x_p), (x_a, y, c_p))) = \\
    add_p(x_p, sg_p(add_g(x_p), (x_a, y)), ng_p(c_p)) = \\
    add_p(x_p, sg_p(x_p, (x_a, y)), -c_p) = \\
    add_p(x_p, -\nabla l(f(x_p, x_a), y), -c_p) = \\
    -c_p - \nabla l(f(x_p, x_a), y)
$$

We can also draw this follows:

\begin{figure}[H]
  \centering
  \[ \input{figures/sg_ng_add.tikz} \]
  \label{fig:sg_ng_add}
  \caption{$$((sg \times ng) \circ add)_p: \mathbb{R}^p \times \mathbb{R}^a \times \mathbb{R} \times \mathbb{R}^p \rightarrow \mathbb{R}^p$$}
\end{figure}

We can build on this to form:

$$
    \lenssig
    {\mathbb{R}^{p} \times \mathbb{R}^{p}}
    {\mathbb{R}^{p} \times \mathbb{R}^{p}}
    {\mathbb{R}^p \times \mathbb{R}^p}
    {\mathbb{R}^a \times \mathbb{R} \times \mathbb{R}^p}
    {(((sg \times ng) \circ add)\times cp)_g}
    {(((sg \times ng) \circ add)\times cp)_p}
$$

where:

$$
    (((g \times ng) \circ add)\times cp)_g(x_p, x'_p) = (x_p,x'_p)
    \\
    (((g \times ng) \circ add)\times cp)_p((x_p, x'_p), (x_a, y, c_p))) =
    (-c_p - \nabla l(f(x_p, x_a), y), x'_p)
$$

We can further build on this to form:

$$
    \lenssig
    {\mathbb{R}^{p} \times \mathbb{R}^{p}}
    {\mathbb{R}^{p} \times \mathbb{R}^{p}}
    {\mathbb{R}^p \times \mathbb{R}^p}
    {\mathbb{R}^a \times \mathbb{R} \times \mathbb{R}^p}
    {((((sg \times ng) \circ add)\times cp) \circ sw)_g}
    {((((sg \times ng) \circ add)\times cp) \circ sw)_p}
$$

where:

$$
    ((((sg \times ng) \circ add)\times cp) \circ sw)_g(x_p, x'_p) = (x_p,x'_p)
    \\
    ((((sg \times ng) \circ add)\times cp) \circ sw)_p((x_p, x'_p), (x_a, y, c_p))) =
    (x'_p, -c_p - \nabla l(f(x_p, x_a), y))
$$


which we can draw as follows:

\begin{figure}[H]
  \centering
  \[ \input{figures/sg_ng_add_cp_sw.tikz} \]
  \label{fig:sg_ng_add_cp_sw}
  \caption{$$((((sg \times ng) \circ add)\times cp) \circ sw)_p: \mathbb{R}^p \times \mathbb{R}^p \times \mathbb{R}^a \times \mathbb{R} \times \mathbb{R}^p \rightarrow \mathbb{R}^p \times \mathbb{R}^p$$}
\end{figure}

Putting it all together we have:

$$
    \lenssig
    {\mathbb{R}^{p} \times \mathbb{R}^{p}}
    {\mathbb{R}^{p} \times \mathbb{R}^{p}}
    {\mathbb{R}^p}
    {\mathbb{R}^a \times \mathbb{R}}
    {(srp \circ ((((sg \times ng) \circ add)\times cp) \circ sw))_g}
    {(srp \circ ((((sg \times ng) \circ add)\times cp) \circ sw))_p}
$$

where:

$$
    (srp \circ ((((sg \times ng) \circ add)\times cp) \circ sw))_g(x_p, x'_p) =
    x_p =
    sm_g((x_p, x'_p))
$$

and we have:

$$
    (srp \circ ((((sg \times ng) \circ add)\times cp) \circ sw))_p((x_p, x'_p), (x_a, y)) = \\

    (x'_p, -x'_p - \nabla l(f(x_p, x_a), y)) = \\

    sm_p((x_p, x'_p), (x_a, y))
$$

which we can draw as follows. Note that $$((((sg \times ng) \circ add)\times cp) \circ sw)_g = id$$:

\begin{figure}[H]
  \centering
  \[
  \input{figures/sm.tikz}
  \]
  \label{fig:sm}
  \caption{$$(srp \circ ((((sg \times ng) \circ add)\times cp) \circ sw))_p = sm_p: \mathbb{R}^p \times \mathbb{R}^p \times \mathbb{R}^a \times \mathbb{R} \rightarrow \mathbb{R}^p \times \mathbb{R}^p$$}
\end{figure}
