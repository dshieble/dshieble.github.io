---
layout: post
title: Supervised Clustering With Kan Extensions
tags: [Clustering, Machine Learning, Extrapolation, Kan Extension, Category Theory, Functorial]
---
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-82391879-1', 'auto');
  ga('send', 'pageview');

</script>

<!--
NOTE: The Kan extension will be formed from 2 sources of structure
  - The original collections of clusterings formed from the base functor
  - The morphisms in the extrapolated to category

NOTE: For the clustering use case, the image of any natural transformation to RanK is itself just the full set of RanK, so the third extrapolation (image of the natural transformation LanK -> RanK) is just RanK
 -->

Clustering algorithms allow us to group points in a dataset together based on some notion of similarity between them. Formally, we can consider a clustering algorithm as mapping a metric space $$(X, d_X)$$ (representing data) to a partitioning of $$X$$.

In most applications of clustering the points in the metric space $$(X, d_X)$$ are grouped together based solely on the distances between the points and the rules embedded within the clustering algorithm itself. This is an unsupervised clustering strategy, since no labels or supervision influence the algorithm output. For example, agglomerative clustering algorithms like HDBSCAN and single linkage partition points in $$X$$ based on graphs formed from the points (vertices) and distances (edges) in $$(X, d_X)$$.

However, there are some circumstances under which we have a few ground truth examples of pre-clustered training datasets and want to learn an algorithm that can cluster new data in as similar of a way as possible. That is, given a collection of tuples
$$\mathbf{S} = \{(X_1, d_{X_1}, P_{X_1}), (X_2, d_{X_2}, P_{X_2}), \cdots, (X_n, d_{X_n}, P_{X_n})\}$$
where $$P_X$$ is a partition of $$X$$ we would like to learn a function $$f: (X, d_X) \rightarrow (X, P_X)$$ such that for each $$(X_i, d_{X_i}, P_{X_i}) \in \mathbf{S}$$ we have $$f(X_i, d_{X_i}) = (X_i, P_{X_i})$$. 

In this blog post we will demonstrate how we can utilize the Kan Extension, a fundamental tool of Category Theory, to solve this problem. 




## Functorial Perspective on Clustering

<!-- TODO: Maybe relax the requirement that D is a subcategory of Met and permit a broader class of clustering functors that extrapolate over??? -->


In order to invoke the Kan Extension for clustering extrapolation we will use a functorial perspective on clustering. We begin with the following definitions. For more information see Section 4 of [Category Theory in Machine Learning](https://arxiv.org/abs/2106.07032):

* In the category $$\mathbf{Met}$$ objects are metric spaces $$(X, d_X)$$ and morphisms are non-expansive maps $$f: (X, d_X) \rightarrow (Y, d_Y)$$ such that $$d_Y(f(x_1), f(x_2)) \leq d_X(x_1, x_2)$$. 
* In the category $$\mathbf{Part}$$ objects are tuples $$(X, P_X)$$ where $$P_X$$ is a partitioning of $$X$$ and morphisms are refinement-preserving maps $$f: (X, P_X) \rightarrow (Y, P_Y)$$ such that if $$\exists S_X \in P_X, x_1, x_2 \in S_X$$ then $$\exists S_Y \in P_Y, f(x_1), f(x_2) \in S_Y$$. 
* Given a subcategory $$\mathbf{D}$$ of $$\mathbf{Met}$$, a clustering functor is a functor $$F: \mathbf{D} \rightarrow \mathbf{Part}$$ that is the identity on the underlying set (i.e. $F$ commutes with the forgetful functor that maps $$(X, d_X)$$ to $$X$$).

We can now frame our goal as follows. Given a category $$\mathbf{D}$$ of metric spaces, a subcategory $$\mathbf{T} \subseteq \mathbf{D}$$ (the training set) and a clustering functor $$K: \mathbf{T} \rightarrow \mathbf{Part}$$, find a $$$$clustering functor $$F: \mathbf{D} \rightarrow \mathbf{Part}$$ that agrees with $$K$$ on $$\mathbf{T}$$.  



## Kan Extensions for Extrapolation

Suppose we have three categories $$A, B, C$$ and two functors $$G: A \rightarrow B, K: A \rightarrow C$$. A Kan Extension is a strategy for constructing a functor $$F: B \rightarrow C$$ from these components in a universal way.


![Kan Extension](/img/kan_extension_pic.png)

Formally, there are two ways that we can construct a Kan Extension:

* The Right Kan Extension $$RanK: B \rightarrow C$$ is the universal functor such that there exists a natural transformation $$\mu: (RanK \circ G) \rightarrow K$$. That is, for any other functor $$M: B \rightarrow C$$ such that there exists a natural transformation $$\lambda_M: (M \circ G) \rightarrow K$$ there exists a natural transformation $$M \rightarrow RanK$$
* The Left Kan Extension $$LanK: B \rightarrow C$$ is the universal functor such that there exists a natural transformation $$\mu: K \rightarrow (LanK \circ G)$$. That is, for any other functor $$M: B \rightarrow C$$ such that there exists a natural transformation $$\lambda_M: K \rightarrow (M \circ G)$$ there exists a natural transformation $$LanK \rightarrow M$$

Intuitively, if we treat $$G$$ as an inclusion of $$A$$ into $$B$$ then the Kan Extension acts as an extrapolation of $$K$$ from $$A$$ to all of $$B$$. That is, both the left and right Kan extensions will be equal (up to natural transformation) to $$K$$ on the image of $$G$$ in $$B$$, and on the rest of $$B$$ the left/right Kan extensions will respectively be the smallest/largest such extrapolations.

For example, suppose we want to use a Kan extension to extrapolate a monotonic function $$K: \mathbb{Z} \rightarrow \mathbb{R}$$ to a function $$F_K: \mathbb{R} \rightarrow \mathbb{R}$$. In this case $$A=\mathbb{Z}, B=C=\mathbb{R}$$ (objects are numbers and morphisms are $$\leq$$) and $$G: \mathbb{Z} \rightarrow \mathbb{R}$$ is the inclusion map. We have that $$LanK: \mathbb{R} \rightarrow \mathbb{R}$$ is simply $$K \circ floor$$ and $$RanK: \mathbb{R} \rightarrow \mathbb{R}$$ is simply $$K \circ ceil$$, where $$floor, ceil$$ are the rounding down and rounding up functions respectively. 


We can use Kan extensions to solve the clustering problem posted above. Suppose $$A=\mathbf{T}$$ is our training set, $$B=\mathbf{D}$$ is our category of metric spaces, and $$C=\mathbf{Part}$$. Then we have:


<!--
$$RanK: \mathbf{D} \rightarrow \mathbf{Part}$$ maps metric spaces to clusterings such that for any metric space $$(X, d_X)$$ in $$\mathbf{D}$$, $$(RanK \circ G)(X, d_X)$$ refines $$K(X, d_X)$$ and for any other $$M$$ that satisfies that property, $$M(X, d_X)$$ refines $$(RanK \circ G)(X, d_X)$$.

$$LanK: \mathbf{D} \rightarrow \mathbf{Part}$$ maps metric spaces to clusterings such that for any metric space $$(X, d_X)$$ in $$\mathbf{D}$$, $$K(X, d_X)$$ refines $$(LanK \circ G)(X, d_X)$$ and for any other $$M$$ that satisfies that property, $$(LanK \circ G)(X, d_X)$$ refines $$M(X, d_X)$$.
 -->

*  $$RanK: \mathbf{D} \rightarrow \mathbf{Part}$$ maps the metric space $$(X, d_X)$$ to the transitive closure of the relation $$R$$ where for $$x_1, x_2 \in X$$ we have $$x_1 R x_2$$ if for any metric space $$(X^{'}, d_{X^{'}}) \in \mathbf{T}$$ and $$f: (X, d_X) \rightarrow (X^{'}, d_{X^{'}})$$ in $$\mathbf{D}$$ it is the case that $$f(x_1), f(x_2)$$ are in the same partition in $$K(X^{'}, d_{X^{'}})$$.
* $$LanK: \mathbf{D} \rightarrow \mathbf{Part}$$ maps the metric space $$(X, d_X)$$ to the transitive closure of the relation $$R$$ where for any metric space $$(X^{'}, d_{X^{'}}) \in \mathbf{T}$$ and $$f: (X^{'}, d_{X^{'}}) \rightarrow (X, d_{X})$$ in $$\mathbf{D}$$, if the points $$x_1^{'}, x_2^{'} \in X^{'}$$ are in the same partition in $$K(X^{'}, d_{X^{'}})$$ we have $$f(x_1^{'}) R f(x_2^{'})$$

Intuitively, $$RanK(X, d_X)$$ is the coarsest (fewest partitions) clustering and $$LanK(X, d_X)$$ is the finest (most partitions) clustering such that $$LanK=RanK=K$$ on metric spaces in $$\mathbf{T}$$ and functoriality over $$\mathbf{D}$$ is satisfied.


We can treat the left and right Kan extensions as algorithms that map training datasets to clustering functors. There are two sources of structure that this algorithm can take advantage of: the functor $$K$$ and the category $$\mathbf{D}$$. The more restrictive that $$\mathbf{D}$$ is (e.g. the more morphisms in $$\mathbf{D}$$) or the larger that $\mathbf{T}$ is (and therefore the more information that is stored in $K$) the more similar that any two functors $$F_1, F_2: \mathbf{D} \rightarrow \mathbf{Part}$$ that satisfy this criterion will be to each other.


## Examples


##### Example 1

<!-- As a concrete example, suppose that we want to determine a clustering of $$(\{x_1,x_2,x_3\}, d)$$ given the observed clusterings $$(\{x_1,x_3\}, d) \rightarrow \{\{x_1\},\{x_3\}\}$$ and $$(\{x_2,x_3\}, d)) \rightarrow \{\{x_2\},\{x_3\}\}$$. We can characterize this in our framework as follows. -->

Suppose
$$\mathbf{D}=\{(\{x_1,x_2\}, d), (\{x_1,x_3\}, d), (\{x_1,x_2,x_3\}, d)\}$$
and
$$\mathbf{T}=\{(\{x_1,x_2\}, d), (\{x_1,x_3\}, d)\}$$
are discrete subcategories of $$\mathbf{Met}$$ and $$K: \mathbf{T} \rightarrow \mathbf{Part}$$ is a clustering functor defined as $$K(\{x_1,x_3\}, d) = \{\{x_1\},\{x_3\}\}, K(\{x_2,x_3\}, d)) = \{\{x_2\},\{x_3\}\}$$. Then:
* Since there are no metric spaces in $$\mathbf{X}$$ with more than $$2$$ points, $$RanK(\{x_1,x_2,x_3\}, d) = \{\{x_1,x_2,x_3\}\}$$
* Since the only points that need to be put together are $$x_1, x_2$$, we have that $$LanK(\{x_1,x_2,x_3\}, d) = \{\{x_1,x_2\}, \{x_3\}\}$$ 



##### Example 2

Suppose
$$\mathbf{D}=\{(\{x_1,x_2,x_3\}, d), \{x_1,x_2\}\}$$
and
$$\mathbf{T}=\{(\{x_1,x_2,x_3\}, d)\}$$
are discrete subcategories of $$\mathbf{Met}$$ and $$K: \mathbf{T} \rightarrow \mathbf{Part}$$ is is a clustering functor defined as $$K(\{x_1,x_2,x_3\}, d) = \{\{x_1, x_2\}, \{x_3\}\}$$. Then:
* Since $$x_2,x_3$$ are not together in $$K(\{x_1,x_2,x_3\}, d)$$, we have that $$RanK(\{x_2,x_3\}, d) = \{\{x_2\}, \{x_3\}\}$$
* Since there are no metric spaces in $$\mathbf{T}$$ with fewer than $$3$$ points, $$LanK(\{x_2,x_3\}, d) = \{\{x_2\},\{x_3\}\}$$



##### Example 3

<!-- TODO: What happens when we need to extend to something that may not exist, like robust single linkage over all of Met?  -->


Given a metric space $$(X, d_X)$$, we can think of its $$\delta$$-Vietoris Rips complex as a graph in which the vertices are $$X$$ and there exists an edge between $$x_1$$ and $$x_2$$ when $$d_X(x_1, x_2) \leq \delta$$. The $$\delta$$-single linkage clustering functor maps a metric space $$(X, d_X)$$ to the connected components of its $$\delta$$-Vietoris Rips complex and the $$(\delta, \sigma)$$-robust single linkage functor maps a metric space $$(X, d_X)$$ to the connected components of the $$\delta$$-Vietoris-Rips complex of $$(X, d_{X}^{\sigma})$$ where:

$$d_{X}^{\sigma}(x_1, x_2) = max(d_X(x_1, x_2), \mu_{X_\sigma}(x_1), \mu_{X_\sigma}(x_2))$$

and $$\mu_{X_\sigma}(x_1)$$ is the distance from $$x_1$$ to its $$\sigma$$th nearest neighbor. Intuitively, robust single linkage reduces the impact of dataset noise by increasing distances in sparse regions of the space. Note that robust single linkage is not a clustering functor on $$\mathbf{Met}$$ because it includes a $$k$$-nearest neighbor computation that is sensitive to the cardinality of $$X$$ but it is a clustering functor on the restriction of $$\mathbf{Met}$$ to its subcategory in which morphisms are restricted to injections.

Now suppose $$\mathbf{D}=\mathbf{Met}$$, $$\mathbf{T}$$ is a subcategory of $$\mathbf{D}$$ in which morphisms are restricted to injections, and $$K: \mathbf{T} \rightarrow \mathbf{Part}$$ is the $$(\delta, \sigma)$$-robust single linkage algorithm. By [Carlsson et. al.](https://arxiv.org/abs/0808.2241), both $$RanK$$ and $$LanK$$ must be $$\delta$$-single linkage functors for some $$\delta_{RanK}, \delta_{LanK}$$. 
* $$\delta_{RanK}$$ must be the largest it can be such that the output of the $$\delta$$-single linkage functor refines the output of the $$(\delta, \sigma)$$-single linkage functor, which is $$\delta$$.
* $$\delta_{LanK}$$ must be the smallest it can be such that the output of the $$\delta$$-single linkage functor is always refined by the output of the $$(\delta, \sigma)$$-single linkage functor, which is $$max(\delta, \sup \{\mu_{X_\sigma}(x) \mid x \in X, (X, d_X) \in \mathbf{T}\})$$.

## References

* Azimuth Blog on [Left Kan Extensions](https://forum.azimuthproject.org/discussion/2267/lecture-50-chapter-3-left-kan-extensions) and [Right Kan Extensions](https://forum.azimuthproject.org/discussion/2271/lecture-51-chapter-3-right-kan-extensions) 
* [Higher Interpolation and Extension for Persistence Modules](https://epubs.siam.org/doi/pdf/10.1137/16M1100472)
* [Persistent Clustering and a Theorem of J. Kleinberg](https://arxiv.org/abs/0808.2241)



<!-- 


 $$\delta_{RanK}$$


smaller distances = more coars/e
NOTE: robust single linkage distances are larger, so the clusterings are finer, so single linkage is the most coarse


RanK is the coarsest (small distances), so $$\delta_{RanK}$$ is as low as possible (aka the highest it can be while still always refining RSL), which is $$\delta$$
 
LanK is the finest (large distances), so $$\delta_{LanK}$$ is as high as possible (aka the lowest it can be while still always being refined by RSL), which is $$\sup \{\mu_{X}^{\sigma}(x) | x \in X, (X, d_X) \in \mathbf{T}\}$$
 -->

<!-- \cite{NIPS2010_4068}.  -->

<!-- 
Given $a_1, a_2 \in (0,1]$, an example of a flat clustering functor on $\metbij$ is the \textbf{r} $\slinkr(a_1, a_2)$ which maps a metric space $(X, d_X)$ to the connected components of the $-log(a_2)$-Vietoris-Rips complex of $(X, d^{a_1}_X)$ where:
%
\begin{gather*}
    % Robust single linkage with mutual reachability distance.
    %
    % There are three main additional parameters that are used by rslink, HDBSCAN, and co-UMAP:
    % (1) spread out distances based on PDF so points in sparser regions have larger distances 
    % (2) remove points from the space that are in sparse regions so that they can't bridge points easily. 
    % (3) remove clusters with fewer than K points
    % Note that (1) and (2) are very similar since expanding the distances around a point is functionally equivalent to removing it from the clustering procedure. Note also that if we include a pruning stage via (3), then we lose functoriality over (a,e), since a more refined set might trigger pruning while a less refined set would not. 
    %
    % NOTE: Increasing \epsilon also increases the distances because it pushes points away more aggressively. \epsilon=1 makes each point's distance equal to its eccentricity. \epsilon=0 makes \mu_{X_\epsilon}(x_1)=0, which removes the robustness entirely. 
   d^{a_1}_X(x_1, x_2) = max(d_X(x_1, x_2), \mu_{X_{a_1}}(x_1), \mu_{X_{a_1}}(x_2))
\end{gather*}
%
 -->



<!-- Suppose $$\mathbf{D}$$ is the subcategory of $$\mathbf{Met}$$ in which morphisms are restricted to injections and $$\mathbf{T}$$ is the subcategory of $$\mathbf{D}$$ in which morphisms are restricted to bijections. Suppose also that $$K: \mathbf{T} \rightarrow \mathbf{Part}$$ is the 
 --> 

<!-- 
are discrete subcategories of $$\mathbf{Met}$$ and $$K: \mathbf{T} \rightarrow \mathbf{Part}$$ is is a clustering functor defined as $$K(\{x_1,x_2,x_3\}, d) = \{\{x_1, x_2\}, \{x_3\}\}$$. Then:
* Since $$x_2,x_3$$ are not together in $$K(\{x_1,x_2,x_3\}, d)$$, we have that $$RanK(\{x_2,x_3\}, d) = \{\{x_2\}, \{x_3\}\}$$
* Since there are no metric spaces in $$\mathbf{T}$$ with fewer than $$3$$ points, $$LanK(\{x_2,x_3\}, d) = \{\{x_2\},\{x_3\}\}$$
 -->

<!-- ### Analysis  -->



<!-- 
 \textbf{Kan-Extension Clustering Extrapolation}
Suppose we have a set of metric spaces $$\mathbf{S}$$:
A = Metric spaces $$(X, d_X) \in \mathbf{S}$$, morphisms are non-expansive bijections 
B = All metric spaces $$(X, d_X) $$, morphisms are non-expansive bijections
C = all clusterings of sets
G: A -> B = inclusion map
K: A -> C = clustering functor


$$RanK: B \rightarrow C$$ maps metric spaces to clusterings such that for any metric space x in $$A$$, $$(RanK \circ G)(X, d_X)$$ refines $$K(X, d_X)$$ and for any other $$M$$ that satisfies that property, $$M(X, d_X)$$ refines $$(RanK \circ G)(X, d_X)$$

$$LanK: B \rightarrow C$$ maps metric spaces to clusterings such that for any metric space m in $$A$$, $$K(X, d_X)$$ refines $$(LanK \circ G)(X, d_X)$$ and for any other $$M$$ that satisfies that property, $$(LanK \circ G)(X, d_X)$$ refines $$M(X, d_X)$$ 

% With natural transformation Ran . G -> K, and for candidate M also M -> Ran
$$RanK: B \rightarrow C$$
\begin{itemize}
\item On metric spaces in $$\mathbf{S}$$, (elements of $$A$$) $$RanK = K$$
\item If $$(X, d_X)$$ is a metric space not in $$\mathbf{S}$$, $$RanK $$ maps $$X$$ to the coarsest clustering such that both (1) functoriality and (2) $$RanK=K$$ on metric spaces in $$\mathbf{S}$$ are satisfied. This is the transitive closure of the relation $$R$$ where for $$x_1, x_2 \in X$$ we have $$x_1 R x_2$$ if for any metric space $$(X^{'}, d_{X^{'}}) \in \mathbf{S}$$ and $$f: (X, d_X) \rightarrow (X^{'}, d_{X^{'}})$$ it is the case that $$f(x_1), f(x_2)$$ are in the same partition in $$K(X^{'}, d_{X^{'}})$$ % Intuitively, we put together every pair of points that we can
\end{itemize}

 
% With natural transformation K -> Lan . G, and for candidate M also Lan -> M
$$LanK: B \rightarrow C$$
\begin{itemize}
\item On metric spaces in $$\mathbf{S}$$ (elements of $$A$$) $$LanK = K$$
\item  If $$(X, d_X)$$ is a metric space not in $$\mathbf{S}$$, $$LanK $$ maps $$X$$ to the finest clustering such that both (1) functoriality and (2) $$LanK=K$$ on metric spaces in $$\mathbf{S}$$ are satisfied. This is the transitive closure of the relation $$R$$ where for any metric space $$(X^{'}, d_{X^{'}}) \in \mathbf{S}$$ and $$f: (X^{'}, d_{X^{'}}) \rightarrow (X, d_{X})$$, if the points $$x^{'}_1, x^{'}_2 \in X^{'}$$ are in the same partition in $$K(X^{'}, d_{X^{'}})$$ we have $$f(x^{'}_1) R f(x^{'}_2)$$  % Intuitively, we put together every pair of points that we have to
\end{itemize}







 And we want to determine the actions of $$Lan_{K_1}(G)$$ and $$Ran_{K_1}(G)$$ on $$X = (\{x_1,x_2\}, d)$$:

\item S
\item 

We can use this as $$\mathbf{D}=\mathbf{Met}$$, $$\mathbf{T}$$ is the discrete category (no non-identity morphisms) $$((\{x_1,x_2\}, d), (\{x_1,x_3\}, d), (\{x_2,x_3\}, d))$$


where
$$d(x_1,x_2) = 1, d(x_1,x_3) = 3, d(x_2,x_3) = 2$$ and the clustering functor $$K: \mathbf{T} \rightarrow \mathbf{Part}$$ is defined as
. Now suppose that we want to use this data to determine the clustering of $$(\{x_1,x_2,x_3\}, d)$$. We can 

The left and right Kan extensions $$LanK$$ and $$RanK$$ as follows. want to determine the actions of $$Lan_{K_1}(G)$$ and $$Ran_{K_1}(G)$$ on $$X = (\{x_1,x_2,x_3\}, d)$$, where $$G$$ is the inclusion map.
  
\begin{itemize}
\item Since there are no metric spaces in $$\mathbf{X}$$with more than $$2$$ points, $$Ran_{K_1}(G)$$ will simply map this to $$\{\{x_1,x_2,x_3\}\}$$
\item Since there are no metric spaces in $$\mathbf{X}$$ ith fewer than $$3$$ points, $$Lan_{K_1}(G)$$ will simply map this to $$\{\{x_2\},\{x_3\}\}$$
\end{itemize}


\begin{itemize}
\item Since $$x_2,x_3$$ are not together in $$K_1(\{x_2,x_3\}, d) $$, $$Ran_{K_1}(G)$$ must map this to $$\{\{x_2\}, \{x_3\}\}$$
\item Since there are no metric spaces in $$\mathbf{X}$$ ith fewer than $$3$$ points, $$Lan_{K_1}(G)$$ will simply map this to $$\{\{x_2\},\{x_3\}\}$$
\end{itemize}



However, it is possible to learn clustering algorithms from data can also use clustering for supervised learning. If we have a collection of labeled gr

and the K-means clustering

A common problem that we encounter when we work with data processing algorithms is extrapolating their results on new data. For example, we may have a set of metric spaces $$S$$ over which we have  -->
