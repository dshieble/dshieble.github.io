---
layout: post
title: Supervised Clustering With Kan Extensions
tags: [Clustering, Machine Learning, Extrapolation, Kan Extension, Category Theory, Functorial]
---
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-82391879-1', 'auto');
  ga('send', 'pageview');

</script>

<!--
NOTE: The Kan extension will be formed from 2 sources of structure
  - The original collections of clusterings formed from the base functor
  - The morphisms in the extrapolated to category
 -->

Clustering algorithms allow us to group points in a dataset together based on some notion of similarity between them. Formally, we can consider a clustering algorithm as mapping a metric space $$(X, d_X)$$ (representing data) to a partitioning of $$X$$.

In most applications of clustering the points in the metric space $$(X, d_X)$$ are grouped together based solely on the distances between the points and the rules embedded within the clustering algorithm itself. This is an unsupervised clustering strategy, since no labels or supervision influence the algorithm output. For example, agglomerative clustering algorithms like HDBSCAN and single linkage partition points in $$X$$ based on graphs formed from the points (vertices) and distances (edges) in $$(X, d_X)$$.

However, there are some circumstances under which we have a few ground truth examples of pre-clustered training datasets and want to learn an algorithm that can cluster new data in as similar of a way as possible. That is, given a collection of tuples
$$\mathbf{S} = \{(X_1, d_{X_1}, P_{X_1}), (X_2, d_{X_2}, P_{X_2}), \cdots, (X_n, d_{X_n}, P_{X_n})\}$$
where $$P_X$$ is a partition of $$X$$ we would like to learn a function $$f: (X, d_X) \rightarrow (X, P_X)$$ such that for each $$(X_i, d_{X_i}, P_{X_i}) \in \mathbf{S}$$ we have $$f(X_i, d_{X_i}) = (X_i, P_{X_i})$$. 

In this blog post we will demonstrate how we can utilize the Kan Extension, a fundamental tool of Category Theory, to solve this problem. 





## Functorial Perspective on Clustering

<!-- TODO: Maybe relax the requirement that D is a subcategory of Met and permit a broader class of clustering functors that extrapolate over??? -->


In order to invoke the Kan Extension for clustering extrapolation we will use a functorial perspective on clustering. We begin with the following definitions. For more information see Section 4 of [Category Theory in Machine Learning](https://arxiv.org/abs/2106.07032):

* In the category $$\mathbf{Met}$$ objects are metric spaces $$(X, d_X)$$ and morphisms are non-expansive maps $$f: (X, d_X) \rightarrow (Y, d_Y)$$ such that $$d_Y(f(x_1), f(x_2)) \leq d_X(x_1, x_2)$$. 
* In the category $$\mathbf{Part}$$ objects are tuples $$(X, P_X)$$ where $$P_X$$ is a partitioning of $$X$$ and morphisms are refinement-preserving maps $$f: (X, P_X) \rightarrow (Y, P_Y)$$ such that if $$\exists S_X \in P_X, x_1, x_2 \in S_X$$ then $$\exists S_Y \in P_Y, f(x_1), f(x_2) \in S_Y$$. 
* Given a subcategory $$\mathbf{D}$$ of $$\mathbf{Met}$$, a clustering functor is a functor $$F: \mathbf{D} \rightarrow \mathbf{Part}$$ that is the identity on the underlying set (i.e. $F$ commutes with the forgetful functor that maps $$(X, d_X)$$ to $$X$$).

We can now frame our goal as follows. Given a category $$\mathbf{D}$$ of metric spaces, a subcategory $$\mathbf{T} \subseteq \mathbf{D}$$ (the training set) and a clustering functor $$K: \mathbf{T} \rightarrow \mathbf{Part}$$, find a $$$$clustering functor $$F: \mathbf{D} \rightarrow \mathbf{Part}$$ that agrees with $$K$$ on $$\mathbf{T}$$. Note that there are two sources of structure that $$F$$ needs to respect: the functor $$K$$ and the category $$\mathbf{D}$$. The more restrictive that $$\mathbf{D}$$ is (e.g. the more morphisms in $$\mathbf{D}$$), the more similar that any two functors $$F_1, F_2: \mathbf{D} \rightarrow \mathbf{Part}$$ that satisfy this criterion will be to each other. 




## Kan Extensions for Extrapolation

<!-- TODO: Also permit the image of the right Kan extension into the left Kan extenion -->


Suppose we have three categories $$A, B, C$$ and two functors $$G: A \rightarrow B, K: A \rightarrow C$$. A Kan Extension is a strategy for constructing a functor $$F: B \rightarrow C$$ from these components in a universal way.


![Kan Extension](/img/kan_extension_pic.png)

Formally, there are two ways that we can construct a Kan Extension:

* The Right Kan Extension $$RanK: B \rightarrow C$$ is the universal functor such that there exists a natural transformation $$\mu: (RanK \circ G) \rightarrow K$$. That is, for any other functor $$M: B \rightarrow C$$ such that there exists a natural transformation $$\lambda_M: (M \circ G) \rightarrow K$$ there exists a natural transformation $$M \rightarrow RanK$$
* The Left Kan Extension $$LanK: B \rightarrow C$$ is the universal functor such that there exists a natural transformation $$\mu: K \rightarrow (LanK \circ G)$$. That is, for any other functor $$M: B \rightarrow C$$ such that there exists a natural transformation $$\lambda_M: K \rightarrow (M \circ G)$$ there exists a natural transformation $$LanK \rightarrow M$$

Intuitively, if we treat $$G$$ as an inclusion of $$A$$ into $$B$$ then the Kan Extension acts as an extrapolation of $$K$$ from $$A$$ to all of $$B$$. That is, both the left and right Kan extensions will be equal (up to natural transformation) to $$K$$ on the image of $$G$$ in $$B$$, and on the rest of $$B$$ the left/right Kan extensions will respectively be the smallest/largest such extrapolations.

For example, suppose we want to use a Kan extension to extrapolate a monotonic function $$K: \mathbb{Z} \rightarrow \mathbb{R}$$ to a function $$F_K: \mathbb{R} \rightarrow \mathbb{R}$$. In this case $$A=\mathbb{Z}, B=C=\mathbb{R}$$ (objects are numbers and morphisms are $$\leq$$) and $$G: \mathbb{Z} \rightarrow \mathbb{R}$$ is the inclusion map. We have that $$LanK: \mathbb{R} \rightarrow \mathbb{R}$$ is simply $$K \circ floor$$ and $$RanK: \mathbb{R} \rightarrow \mathbb{R}$$ is simply $$K \circ ceil$$, where $$floor, ceil$$ are the rounding down and rounding up functions respectively. 


We can use Kan extensions to solve the clustering problem posted above. Suppose $$A=\mathbf{T}$$ is our training set, $$B=\mathbf{D}$$ is our category of metric spaces, and $$C=\mathbf{Part}$$. Then we have:

* $$RanK: \mathbf{D} \rightarrow \mathbf{Part}$$ maps metric spaces to clusterings such that for any metric space $$(X, d_X)$$ in $$\mathbf{D}$$, $$(RanK \circ G)(X, d_X)$$ refines $$K(X, d_X)$$ and for any other $$M$$ that satisfies that property, $$M(X, d_X)$$ refines $$(RanK \circ G)(X, d_X)$$. This is the transitive closure of the relation $$R$$ where for $$x_1, x_2 \in X$$ we have $$x_1 R x_2$$ if for any metric space $$(X^{'}, d_{X^{'}}) \in \mathbf{T}$$ and $$f: (X, d_X) \rightarrow (X^{'}, d_{X^{'}})$$ in $$\mathbf{D}$$ it is the case that $$f(x_1), f(x_2)$$ are in the same partition in $$K(X^{'}, d_{X^{'}})$$.
* $$LanK: \mathbf{D} \rightarrow \mathbf{Part}$$ maps metric spaces to clusterings such that for any metric space $$(X, d_X)$$ in $$\mathbf{D}$$, $$K(X, d_X)$$ refines $$(LanK \circ G)(X, d_X)$$ and for any other $$M$$ that satisfies that property, $$(LanK \circ G)(X, d_X)$$ refines $$M(X, d_X)$$. This is the transitive closure of the relation $$R$$ where for any metric space $$(X^{'}, d_{X^{'}}) \in \mathbf{T}$$ and $$f: (X^{'}, d_{X^{'}}) \rightarrow (X, d_{X})$$ in $$\mathbf{D}$$, if the points $$x_1^{'}, x_2^{'} \in X^{'}$$ are in the same partition in $$K(X^{'}, d_{X^{'}})$$ we have $$f(x_1^{'}) R f(x_2^{'})$$

Intuitively, $$RanK(X, d_X)$$ is the coarsest (fewest partitions) clustering and $$LanK(X, d_X)$$ is the finest (most partitions) clustering such that both (1) functoriality and (2) $$LanK=RanK=K$$ on metric spaces in $$\mathbf{T}$$ are satisfied. 

As a concrete example, suppose that we want to determine a clustering of $$(\{x_1,x_2,x_3\}, d)$$ given the observed clusterings $$(\{x_1,x_3\}, d) \rightarrow \{\{x_1\},\{x_3\}\}$$ and $$(\{x_2,x_3\}, d)) \rightarrow \{\{x_2\},\{x_3\}\}$$. We can characterize this in our framework as follows. Suppose $$\mathbf{D}=\mathbf{Met}$$, $$\mathbf{T}$$ is the discrete subcategory of $$\mathbf{D}$$ where objects are limited to $$((\{x_1,x_2\}, d)$$ and $$(\{x_1,x_3\}, d)$$, and $$K: \mathbf{T} \rightarrow \mathbf{Part}$$ is the clustering functor defined as $$K(\{x_1,x_3\}, d) = \{\{x_1\},\{x_3\}\}, K(\{x_2,x_3\}, d)) = \{\{x_2\},\{x_3\}\}$$. Then:
* Since there are no metric spaces in $$\mathbf{X}$$ with more than $$2$$ points, $$RanK(\{x_1,x_2,x_3\}, d) = \{\{x_1,x_2,x_3\}\}$$
* Since the only points that need to be put together are $$x_1, x_2$$, we have that $$LanK(\{x_1,x_2,x_3\}, d) = \{\{x_1,x_2\}, \{x_3\}\}$$ 

Now suppose instead that $$\mathbf{T} = \{(\{x_1,x_2,x_3\}, d)\}$$, $$K(\{x_1,x_2,x_3\}, d) = \{\{x_1, x_2\}, \{x_3\}\}$$ and we want to determine a clustering of $$(\{x_1,x_2\}, d)$$. 
* Since $$x_2,x_3$$ are not together in $$K(\{x_1,x_2,x_3\}, d)$$, we have that $$RanK(\{x_1,x_2\}, d) = \{\{x_2\}, \{x_3\}\}$$
* Since there are no metric spaces in $$\mathbf{T}$$ with fewer than $$3$$ points, $$LanK(\{x_1,x_2\}, d) = \{\{x_2\},\{x_3\}\}$$




## References

* Azimuth Blog on [Left Kan Extensions](https://forum.azimuthproject.org/discussion/2267/lecture-50-chapter-3-left-kan-extensions) and [Right Kan Extensions](https://forum.azimuthproject.org/discussion/2271/lecture-51-chapter-3-right-kan-extensions) 
* [Higher Interpolation and Extension for Persistence Modules](https://epubs.siam.org/doi/pdf/10.1137/16M1100472)



<!-- 
 \textbf{Kan-Extension Clustering Extrapolation}
Suppose we have a set of metric spaces $$\mathbf{S}$$:
A = Metric spaces $$(X, d_X) \in \mathbf{S}$$, morphisms are non-expansive bijections 
B = All metric spaces $$(X, d_X) $$, morphisms are non-expansive bijections
C = all clusterings of sets
G: A -> B = inclusion map
K: A -> C = clustering functor


$$RanK: B \rightarrow C$$ maps metric spaces to clusterings such that for any metric space x in $$A$$, $$(RanK \circ G)(X, d_X)$$ refines $$K(X, d_X)$$ and for any other $$M$$ that satisfies that property, $$M(X, d_X)$$ refines $$(RanK \circ G)(X, d_X)$$

$$LanK: B \rightarrow C$$ maps metric spaces to clusterings such that for any metric space m in $$A$$, $$K(X, d_X)$$ refines $$(LanK \circ G)(X, d_X)$$ and for any other $$M$$ that satisfies that property, $$(LanK \circ G)(X, d_X)$$ refines $$M(X, d_X)$$ 

% With natural transformation Ran . G -> K, and for candidate M also M -> Ran
$$RanK: B \rightarrow C$$
\begin{itemize}
\item On metric spaces in $$\mathbf{S}$$, (elements of $$A$$) $$RanK = K$$
\item If $$(X, d_X)$$ is a metric space not in $$\mathbf{S}$$, $$RanK $$ maps $$X$$ to the coarsest clustering such that both (1) functoriality and (2) $$RanK=K$$ on metric spaces in $$\mathbf{S}$$ are satisfied. This is the transitive closure of the relation $$R$$ where for $$x_1, x_2 \in X$$ we have $$x_1 R x_2$$ if for any metric space $$(X^{'}, d_{X^{'}}) \in \mathbf{S}$$ and $$f: (X, d_X) \rightarrow (X^{'}, d_{X^{'}})$$ it is the case that $$f(x_1), f(x_2)$$ are in the same partition in $$K(X^{'}, d_{X^{'}})$$ % Intuitively, we put together every pair of points that we can
\end{itemize}

 
% With natural transformation K -> Lan . G, and for candidate M also Lan -> M
$$LanK: B \rightarrow C$$
\begin{itemize}
\item On metric spaces in $$\mathbf{S}$$ (elements of $$A$$) $$LanK = K$$
\item  If $$(X, d_X)$$ is a metric space not in $$\mathbf{S}$$, $$LanK $$ maps $$X$$ to the finest clustering such that both (1) functoriality and (2) $$LanK=K$$ on metric spaces in $$\mathbf{S}$$ are satisfied. This is the transitive closure of the relation $$R$$ where for any metric space $$(X^{'}, d_{X^{'}}) \in \mathbf{S}$$ and $$f: (X^{'}, d_{X^{'}}) \rightarrow (X, d_{X})$$, if the points $$x^{'}_1, x^{'}_2 \in X^{'}$$ are in the same partition in $$K(X^{'}, d_{X^{'}})$$ we have $$f(x^{'}_1) R f(x^{'}_2)$$  % Intuitively, we put together every pair of points that we have to
\end{itemize}







 And we want to determine the actions of $$Lan_{K_1}(G)$$ and $$Ran_{K_1}(G)$$ on $$X = (\{x_1,x_2\}, d)$$:

\item S
\item 

We can use this as $$\mathbf{D}=\mathbf{Met}$$, $$\mathbf{T}$$ is the discrete category (no non-identity morphisms) $$((\{x_1,x_2\}, d), (\{x_1,x_3\}, d), (\{x_2,x_3\}, d))$$


where
$$d(x_1,x_2) = 1, d(x_1,x_3) = 3, d(x_2,x_3) = 2$$ and the clustering functor $$K: \mathbf{T} \rightarrow \mathbf{Part}$$ is defined as
. Now suppose that we want to use this data to determine the clustering of $$(\{x_1,x_2,x_3\}, d)$$. We can 

The left and right Kan extensions $$LanK$$ and $$RanK$$ as follows. want to determine the actions of $$Lan_{K_1}(G)$$ and $$Ran_{K_1}(G)$$ on $$X = (\{x_1,x_2,x_3\}, d)$$, where $$G$$ is the inclusion map.
  
\begin{itemize}
\item Since there are no metric spaces in $$\mathbf{X}$$with more than $$2$$ points, $$Ran_{K_1}(G)$$ will simply map this to $$\{\{x_1,x_2,x_3\}\}$$
\item Since there are no metric spaces in $$\mathbf{X}$$ ith fewer than $$3$$ points, $$Lan_{K_1}(G)$$ will simply map this to $$\{\{x_2\},\{x_3\}\}$$
\end{itemize}


\begin{itemize}
\item Since $$x_2,x_3$$ are not together in $$K_1(\{x_2,x_3\}, d) $$, $$Ran_{K_1}(G)$$ must map this to $$\{\{x_2\}, \{x_3\}\}$$
\item Since there are no metric spaces in $$\mathbf{X}$$ ith fewer than $$3$$ points, $$Lan_{K_1}(G)$$ will simply map this to $$\{\{x_2\},\{x_3\}\}$$
\end{itemize}



However, it is possible to learn clustering algorithms from data can also use clustering for supervised learning. If we have a collection of labeled gr

and the K-means clustering

A common problem that we encounter when we work with data processing algorithms is extrapolating their results on new data. For example, we may have a set of metric spaces $$S$$ over which we have  -->
