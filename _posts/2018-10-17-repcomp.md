---
layout: post
title: A New Python Package for Comparing Trained Embedding Models
tags: [Embeddings, Machine Learning, ML, Python, Comparison, Neural Network, Word Vector]
---
<script> 
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-82391879-1', 'auto');
  ga('send', 'pageview');

</script>


When I'm building models, I frequently run into situations where I've trained multiple models over a few datasets or tasks and I'm curious about how they compare. For instance, it's intuitively clear that if I train the same neural network over the same dataset several times, each of the trained networks will be "similar" to each other. In contrast, if I train that same neural network over a different dataset with a different objective, the new network will be "different." For example, we intuitively understand that word embeddings trained over Twitter data will behave differently from word embeddings trained over Wikipedia data. However, it's often unclear how to quantify this difference.

To solve this problem, I recently published `repcomp`, a python package for comparing embeddings. `repcomp` supports the following embedding comparison approaches:

* Nearest Neighbors: Fetch the nearest neighbor set of each entity according to embedding distances, and compare model A's neighbor sets to model B's neighbor sets.
* Canonical Correlation: Treat embedding components as observations of random variables and compute the canonical correlations between model A and model B. 
* Unit Match: Form a unit-to-unit matching between model A's embedding components and model B's embedding components and measure the correlations of the matched units.

You can install repcomp from pip 

```
pip install repcomp
```

We can use `repcomp` to easily compare two word embedding models that have been pre-trained on Twitter and Wikipedia data:

```python
  import gensim.downloader as api
  import numpy as np
  from repcomp.comparison import NeighborsComparison

  # Load word vectors from gensim
  glove_wiki_50 = api.load("glove-wiki-gigaword-50")
  glove_twitter_50 = api.load("glove-twitter-50")

  # Build the embedding matrices over the shared vocabularies
  shared_vocab = set(glove_wiki_50.vocab.keys()).intersection(
    set(glove_twitter_50.vocab.keys()))
  glove_wiki_50_vectors = np.vstack([glove_wiki_50.get_vector(word) for word in shared_vocab])
  glove_twitter_50_vectors = np.vstack([glove_twitter_50.get_vector(word) for word in shared_vocab])

  # Run the comparison
  comparator = NeighborsComparison()
  print("The neighbors similarity between glove-wiki-gigaword-50 and glove-twitter-50 is {}".format(
    comparator.run_comparison(glove_wiki_50_vectors, glove_twitter_50_vectors)["similarity"]))
```

We can use also use `repcomp` to:
* Quantify the impact of changing a hyperparameter on an embedding model. In [this notebook](https://github.com/dshieble/RepresentationComparison/tree/master/experiments/Movie_Embedding_Experiment) we use matrix factorization to train a couple of movie embedding models based on the MovieLens dataset, and then use embedding comparison to see how changing the value of hyperparameters affects the learned embedding spaces.
* Measure the difference between the representations that different neural network architectures earn. In this notebook, we compare the final-layer embeddings for Imagenet-trained VGG16, VGG19, and InceptionV3 models


If you're interested in contributing to `repcomp`, please feel free to open up a Pull Request [here](https://github.com/dshieble/RepresentationComparison)!

