<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Dan Shiebler</title>
    <description>Thoughts on Math and ML</description>
    <link>http://localhost:4000</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml" />
    
      <item>
        <title>Optimizers as Dynamical Systems</title>
        <description>
          
          The ideas in this post were hashed out during a series of discussions between myself and Bruno Gavranović Consider a system for forecasting a time series in \(\mathbb{R}\) based on a vector of features in \(\mathbb{R}^a\). At each time \(t\) this system will use the state of the world (represented...
        </description>
        <pubDate>Tue, 12 Oct 2021 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2021-10-12-dynamic/</link>
        <guid isPermaLink="true">http://localhost:4000/2021-10-12-dynamic/</guid>
      </item>
    
      <item>
        <title>Supervised Clustering With Kan Extensions</title>
        <description>
          
          Clustering algorithms allow us to group points in a dataset together based on some notion of similarity between them. Formally, we can consider a clustering algorithm as mapping a metric space \((X, d_X)\) (representing data) to a partitioning of \(X\). In most applications of clustering the points in the metric...
        </description>
        <pubDate>Sun, 25 Jul 2021 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2021-07-25-kan/</link>
        <guid isPermaLink="true">http://localhost:4000/2021-07-25-kan/</guid>
      </item>
    
      <item>
        <title>Transformation Invariant Continuous Optimization Algorithms</title>
        <description>
          
          Continuous Optimization Algorithms Suppose we have a function \(l: \mathbb{R}^n \rightarrow \mathbb{R}\) that we want to minimize. A popular algorithm for accomplishing this is gradient descent, which is an iterative algorithm in which we pick a step size \(\alpha\) and a starting point \(x_0 \in \mathbb{R}^n\) and repeatedly iterate \(x_{t+\alpha}...
        </description>
        <pubDate>Thu, 15 Apr 2021 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2021-04-15-optimizers/</link>
        <guid isPermaLink="true">http://localhost:4000/2021-04-15-optimizers/</guid>
      </item>
    
      <item>
        <title>Gradient Descent Is Euler's Method</title>
        <description>
          
          Gradient Descent Gradient descent is a technique for iteratively minimizing a convex function \(f: \mathbb{R}^n \rightarrow \mathbb{R}\) by repeatedly taking steps along its gradient. We define the gradient of \(f\) to be the unique function \(\nabla f\) that satisfies: \[lim_{p \rightarrow 0} \frac{f(x+p) - f(x) - \nabla f(x)^{T}p}{\|p\|} = 0\]...
        </description>
        <pubDate>Sat, 28 Nov 2020 00:00:00 -0500</pubDate>
        <link>http://localhost:4000/2020-11-28-gradient-descent/</link>
        <guid isPermaLink="true">http://localhost:4000/2020-11-28-gradient-descent/</guid>
      </item>
    
      <item>
        <title>Stability of Mapper Graph Invariants</title>
        <description>
          
          Introduction The Mapper algorithm is a useful tool for identifying patterns in a large dataset by generating a graph summary. We can describe the Mapper algorithm as constructing a discrete approximation of the Reeb graph: Suppose we have a manifold \(\mathbf{X}\) equipped with a distance metric \(d_{\mathbf{X}}\) (such as a...
        </description>
        <pubDate>Mon, 02 Nov 2020 00:00:00 -0500</pubDate>
        <link>http://localhost:4000/2020-11-02-mapper-stability/</link>
        <guid isPermaLink="true">http://localhost:4000/2020-11-02-mapper-stability/</guid>
      </item>
    
      <item>
        <title>Compositionality and Functoriality in Machine Learning</title>
        <description>
          
          Introduction At the heart of Machine Learning is data. In all Machine Learning problems, we use data generated by some process in order to make inferences about that process. In the most general case, we know little to nothing about the data-generating process, and the data itself is just a...
        </description>
        <pubDate>Fri, 02 Oct 2020 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2020-10-02-functoriality/</link>
        <guid isPermaLink="true">http://localhost:4000/2020-10-02-functoriality/</guid>
      </item>
    
      <item>
        <title>PCA vs Laplacian Eigenmaps</title>
        <description>
          
          At first glance, PCA and Laplacian Eigenmaps seem both very similar. We can view both algorithms as constructing a graph from our data, choosing a matrix to represent this graph, computing the eigenvectors of this matrix, and then using these eigenvectors to determine low-dimensionality embeddings of our data. However, the...
        </description>
        <pubDate>Sat, 09 May 2020 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2020-05-09-pca-laplacian/</link>
        <guid isPermaLink="true">http://localhost:4000/2020-05-09-pca-laplacian/</guid>
      </item>
    
      <item>
        <title>Learning Complexity and Generalization Bounds</title>
        <description>
          
          In a typical supervised learning setting, we are given access to a dataset of samples \(S = (X_1, y_1), (X_2, y_2), ..., (X_n, y_n)\) which we assume are drawn from a distribution \(\mathcal{D}\) over \(\textbf{X} \times \textbf{y}\). For simplicity, we will assume that \(\mathbf{X}\) is either the space \(\{0,1\}^n\) or...
        </description>
        <pubDate>Thu, 30 Jan 2020 00:00:00 -0500</pubDate>
        <link>http://localhost:4000/2020-01-30-learning-complexity/</link>
        <guid isPermaLink="true">http://localhost:4000/2020-01-30-learning-complexity/</guid>
      </item>
    
      <item>
        <title>Models of Learning</title>
        <description>
          
          Machine Learning researchers have a tough time agreeing on the best formulations for the problems they face. Even within the relatively well-defined setting of supervised learning, there are lots of ways to express the nature of the problem. At a very high level, we can express supervised learning as a...
        </description>
        <pubDate>Thu, 23 Jan 2020 00:00:00 -0500</pubDate>
        <link>http://localhost:4000/2020-01-23-learning-models/</link>
        <guid isPermaLink="true">http://localhost:4000/2020-01-23-learning-models/</guid>
      </item>
    
      <item>
        <title>White Noise is Pretty Weird</title>
        <description>
          
          I recently went off on a tangent trying to figure out how white noise works, and I found that there is a lot of strangeness to it that may not be apparent at a first glance. The content in this post is primarily from: This stackexchange answer This stackexchange answer...
        </description>
        <pubDate>Thu, 09 Jan 2020 00:00:00 -0500</pubDate>
        <link>http://localhost:4000/2020-01-09-white-noise/</link>
        <guid isPermaLink="true">http://localhost:4000/2020-01-09-white-noise/</guid>
      </item>
    
      <item>
        <title>What are you modeling?</title>
        <description>
          
          In this post, we will explore how Discriminative/Generative and Frequentist/Bayesian algorithms make different decisions about what variables to model probabilistically. There are many ways to characterize Machine Learning algorithms. This is a direct consequence of the rich history, broad applicability and interdisciplinary nature of the field. One of the clearest...
        </description>
        <pubDate>Fri, 03 Jan 2020 00:00:00 -0500</pubDate>
        <link>http://localhost:4000/2020-01-03-modeling-strategies/</link>
        <guid isPermaLink="true">http://localhost:4000/2020-01-03-modeling-strategies/</guid>
      </item>
    
      <item>
        <title>Compositional Structures in Machine Learning</title>
        <description>
          
          As researchers apply Machine Learning to increasingly complex tasks, there is mounting interest in strategies for combining multiple simple models into more powerful algorithms. In this post we will explore some of these techniques. We will use a little bit of language from Category Theory, but not much. In the...
        </description>
        <pubDate>Thu, 14 Nov 2019 00:00:00 -0500</pubDate>
        <link>http://localhost:4000/2019-11-14-mlcomposition/</link>
        <guid isPermaLink="true">http://localhost:4000/2019-11-14-mlcomposition/</guid>
      </item>
    
      <item>
        <title>Some Thoughts on ICLR 2019</title>
        <description>
          
          I recently attended ICLR 2019 in New Orleans, and I was lucky to have the opportunity to show off our paper on a novel attention module and image understanding dataset. I really enjoyed the entire conference, and I thought I’d share brief overviews of two of my favorite presentations from...
        </description>
        <pubDate>Sat, 01 Jun 2019 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2019-06-01-iclr/</link>
        <guid isPermaLink="true">http://localhost:4000/2019-06-01-iclr/</guid>
      </item>
    
      <item>
        <title>My solutions to Bartosz Milewski's &quot;Category Theory for Programmers&quot;</title>
        <description>
          
          I recently worked through Bartosz Milewski’s excellent free book “Category Theory for Programmers.” The book is available online here and here. I had an awesome time reading the book and learning about Category Theory so I figured I’d post my solutions to the book problems online to make it easier...
        </description>
        <pubDate>Sat, 10 Nov 2018 00:00:00 -0500</pubDate>
        <link>http://localhost:4000/2018-11-10-category-solutions/</link>
        <guid isPermaLink="true">http://localhost:4000/2018-11-10-category-solutions/</guid>
      </item>
    
      <item>
        <title>Introducing repcomp - A Python Package for Comparing Trained Embedding Models</title>
        <description>
          
          When I’m building models, I frequently run into situations where I’ve trained multiple models over a few datasets or tasks and I’m curious about how they compare. For instance, it’s clear that if I train two word vector models on random subsets of Wikipedia, the trained models will be “similar”...
        </description>
        <pubDate>Wed, 17 Oct 2018 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2018-10-17-repcomp/</link>
        <guid isPermaLink="true">http://localhost:4000/2018-10-17-repcomp/</guid>
      </item>
    
      <item>
        <title>My Thoughts on KDD 2018</title>
        <description>
          
          Last week I was at KDD 2018 in London. This was my first time at KDD, and I had the opportunity to present our paper on embeddings at the Common Model Infrastructure workshop. I was really impressed by both the workshops and the main program, and I thought I’d share...
        </description>
        <pubDate>Fri, 31 Aug 2018 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2018-08-31-kdd/</link>
        <guid isPermaLink="true">http://localhost:4000/2018-08-31-kdd/</guid>
      </item>
    
      <item>
        <title>Representing Graphs with Low Dimensional Matrix Factorization for Fun and Profit</title>
        <description>
          
          A solid laptop computer in 2018 has about 1TB (1000GB) of disk space, and the capability to store about 16GB of memory in RAM. In comparison, internet users in the United States generate about 3000TB of data every minute 1. An enormous amount of this data takes the form of...
        </description>
        <pubDate>Fri, 30 Mar 2018 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2018-03-30-embeddings/</link>
        <guid isPermaLink="true">http://localhost:4000/2018-03-30-embeddings/</guid>
      </item>
    
      <item>
        <title>Don't trust data too much</title>
        <description>
          
          Introduction There’s a famous scene in the HBO show “The Wire” where the unscrupulous Deputy Commissioner Rawls is addressing the police colonels and majors, and he says: Gentlemen, the word from on high is that the felony rates district by district will decline by five percent before the end of...
        </description>
        <pubDate>Sun, 29 Oct 2017 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2017-10-29-lying-with-data/</link>
        <guid isPermaLink="true">http://localhost:4000/2017-10-29-lying-with-data/</guid>
      </item>
    
      <item>
        <title>R and R^2, the relationship between correlation and the coefficient of determination.</title>
        <description>
          
          There are 2 closely related quantities in statistics - correlation (often referred to as \(R\)) and the coefficient of determination (often referred to as \(R^2\)). Today we’ll explore the nature of the relationship between \(R\) and \(R^2\), go over some common use cases for each statistic and address some misconceptions....
        </description>
        <pubDate>Sun, 25 Jun 2017 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2017-06-25-metrics/</link>
        <guid isPermaLink="true">http://localhost:4000/2017-06-25-metrics/</guid>
      </item>
    
      <item>
        <title>Understanding Neural Networks with Layerwise Relevance Propagation and Deep Taylor Series</title>
        <description>
          
          Deep neural networks are some of the most powerful learning algorithms that have ever been developed. Unfortunately, they are also some of the most complex. The hierarchical non-linear transformations that neural networks apply to data can be nearly impossible to understand. This problem is exacerbated by the non-determinism of neural...
        </description>
        <pubDate>Sun, 16 Apr 2017 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2017-04-16-deep-taylor-lrp/</link>
        <guid isPermaLink="true">http://localhost:4000/2017-04-16-deep-taylor-lrp/</guid>
      </item>
    
  </channel>
</rss>
